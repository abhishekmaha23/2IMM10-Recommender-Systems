{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uj4T8PEHGbMF"
   },
   "source": [
    "# Assignment 2\n",
    "## Question 1: Siamese networks & one-shot learning (7pt)\n",
    "The Cifar-100 dataset is similar to the Cifar-10 dataset. It also consists of 60,000 32x32 RGB images, but they are distributed over 100 classes instead of 10. Thus, each class has much fewer examples, only 500 training images and 100 testing images per class. For more info about the dataset, see https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "\n",
    "*HINT: Import the Cifar-100 dataset directly from Keras, no need to download it from the website. Use* `label_mode=\"fine\"`\n",
    "\n",
    "### Task 1.1: Siamese network\n",
    "**a)**\n",
    "* Train a Siamese Network on the first 80 classes of (the training set of) Cifar-100, i.e. let the network predict the probability that two input images are from the same class. Use 1 as a target for pairs of images from the same class (positive pairs), and 0 for pairs of images from different classes (negative pairs). Randomly select image pairs from Cifar-100, but make sure you train on as many positive pairs as negative pairs.\n",
    "\n",
    "* Evaluate the performance of the network on 20-way one-shot learning tasks. Do this by generating 250 random tasks and obtain the average accuracy for each evaluation round. Use the remaining 20 classes that were not used for training. The model should perform better than random guessing.\n",
    "\n",
    "For this question you may ignore the test set of Cifar-100; it suffices to use only the training set and split this, using the first 80 classes for training and the remaining 20 classes for one-shot testing.\n",
    "\n",
    "*HINT: First sort the data by their labels (see e.g.* `numpy.argsort()`*), then reshape the data to a shape of* `(n_classes, n_examples, width, height, depth)`*, similar to the Omniglot data in Practical 4. It is then easier to split the data by class, and to sample positive and negative images pairs for training the Siamese network.*\n",
    "\n",
    "*NOTE: do not expect the one-shot accuracy for Cifar-100 to be similar to that accuracy for Omniglot; a lower accuracy can be expected. However, accuracy higher than random guess is certainly achievable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5XlMo9yMvf5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar100\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Concatenate\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "MaoAaEv17v6k",
    "outputId": "36d08750-96ff-4d28-e946-b81fa7648e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 10s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# === add code here ===\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=\"fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iiaEnHNMMvgA",
    "outputId": "143de20e-3417-4523-b167-652814b09027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "2JdsjUSzMvgD",
    "outputId": "3f1d6dfb-f8c1-48ab-90a7-3c5e3937bcbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mgoiy9tqMvgF"
   },
   "outputs": [],
   "source": [
    "class_labels = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
    "    'worm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "6aVrASEXMvgH",
    "outputId": "64b5dfef-28f8-4ac6-d3d5-5b96ca13d3f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: leopard\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_id = 5000  # pick any integer from 0 to 49999 to visualize a training example\n",
    "example = x_train[example_id]\n",
    "label = y_train[example_id]\n",
    "print(\"Class label:\", class_labels[label])\n",
    "plt.imshow(example)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "coyB9jvFMvgJ",
    "outputId": "9758261f-6d4b-4304-f4fb-644e081ae1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000,)\n",
      "(10000, 32, 32, 3)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "img_rows,img_cols,chns = 32, 32, 3\n",
    "n_classes = 100\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, chns)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, chns)\n",
    "input_shape = (img_rows, img_cols, chns)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gNBIWXg9MvgM",
    "outputId": "1cd2e86b-c2b7-4f14-d5d0-2b84f83b9e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42957  2126 43210 ... 42095 26322 21122]\n",
      "[9221 1027 2426 ... 8387 9860 9153]\n"
     ]
    }
   ],
   "source": [
    "indexes_train = y_train.argsort()\n",
    "indexes_test = y_test.argsort()\n",
    "print(indexes_train)\n",
    "print(indexes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "_Y6mh6ZYMvgP",
    "outputId": "19cbccf9-3199-479e-876b-8a668a9172e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000,)\n",
      "(10000, 32, 32, 3)\n",
      "(10000,)\n",
      "Class label: spider\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_sort = x_train[indexes_train]\n",
    "y_train_sort = y_train[indexes_train]\n",
    "\n",
    "x_test_sort = x_test[indexes_test]\n",
    "y_test_sort = y_test[indexes_test]\n",
    "\n",
    "print(x_train_sort.shape)\n",
    "print(y_train_sort.shape)\n",
    "print(x_test_sort.shape)\n",
    "print(y_test_sort.shape)\n",
    "\n",
    "example_id = 39999  # pick any integer from 0 to 49999 to visualize a training example\n",
    "example = x_train_sort[example_id]\n",
    "label = y_train_sort[example_id]\n",
    "print(\"Class label:\", class_labels[label])\n",
    "plt.imshow(example)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCsrdPYcMvgU"
   },
   "outputs": [],
   "source": [
    "def group(a, b):\n",
    "    # Get argsort indices, to be used to sort a and b in the next steps\n",
    "    sidx = b.argsort(kind='mergesort')\n",
    "    a_sorted = a[sidx]\n",
    "    b_sorted = b[sidx]\n",
    "\n",
    "    # Get the group limit indices (start, stop of groups)\n",
    "    cut_idx = np.flatnonzero(np.r_[True,b_sorted[1:] != b_sorted[:-1],True])\n",
    "\n",
    "    # Create cut indices for all unique IDs in b\n",
    "    n = b_sorted[-1]+2\n",
    "    cut_idxe = np.full(n, cut_idx[-1], dtype=int)\n",
    "\n",
    "    insert_idx = b_sorted[cut_idx[:-1]]\n",
    "    cut_idxe[insert_idx] = cut_idx[:-1]\n",
    "    cut_idxe = np.minimum.accumulate(cut_idxe[::-1])[::-1]\n",
    "\n",
    "    # Split input array with those start, stop ones\n",
    "    out = [a_sorted[i:j] for i,j in zip(cut_idxe[:-1],cut_idxe[1:])]\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Zf0wCf1nMvgW",
    "outputId": "37d7627a-ddd4-4191-d68b-1ff4f6f2ea3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 500, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train_shape = group(x_train_sort, y_train_sort)\n",
    "x_test_shape = group(x_test, y_test)\n",
    "print(x_train_shape.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "B8u0pBHZMvgZ",
    "outputId": "ffa586e9-8a77-4cfd-9f74-4ea6fd66309a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 500, 32, 32, 3)\n",
      "(40000,)\n",
      "(20, 500, 32, 32, 3)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train_samples = x_train_shape[:80]\n",
    "y_train_samples = y_train_sort[:40000]\n",
    "\n",
    "x_train_rem_samples = x_train_shape[80:]\n",
    "y_train_rem_samples = y_train_sort[40000:]\n",
    "\n",
    "print(x_train_samples.shape)\n",
    "print(y_train_samples.shape)\n",
    "print(x_train_rem_samples.shape)\n",
    "print(y_train_rem_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1212
    },
    "colab_type": "code",
    "id": "SE5d-Oq7Mvgb",
    "outputId": "bde65b12-d5f5-4864-c210-ebbf933b557c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 256)         295168    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              4198400   \n",
      "=================================================================\n",
      "Total params: 4,722,176\n",
      "Trainable params: 4,719,488\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 4096)         4722176     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4096)         0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,726,273\n",
      "Trainable params: 4,723,585\n",
      "Non-trainable params: 2,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "# build convnet to use in each siamese 'leg'\n",
    "convnet = Sequential()\n",
    "\n",
    "convnet.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "\n",
    "convnet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "\n",
    "convnet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "\n",
    "convnet.add(Conv2D(256, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "\n",
    "convnet.add(Dense(4096, activation=\"sigmoid\", kernel_regularizer=l2(1e-3)))\n",
    "convnet.summary()\n",
    "\n",
    "# encode each of the two inputs into a vector with the convnet\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "\n",
    "# merge two encoded inputs with the L1 distance between them, and connect to prediction output layer\n",
    "L1_distance = lambda x: K.abs(x[0]-x[1])\n",
    "both = Lambda(L1_distance)([encoded_l, encoded_r])\n",
    "prediction = Dense(1, activation='sigmoid')(both)\n",
    "siamese_net = Model(inputs=[left_input,right_input], outputs=prediction)\n",
    "\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRpbGhM4Mvgd"
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size, X):\n",
    "    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = np.random.choice(n_classes, size=(batch_size,), replace=False)\n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "    pairs = [np.zeros((batch_size, h, w, d)) for i in range(2)]\n",
    "    # initialize vector for the targets, and make one half of it '1's, so 2nd half of batch has same class\n",
    "    targets = np.zeros((batch_size,))\n",
    "    targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = np.random.randint(0, n_examples)\n",
    "        pairs[0][i, :, :, :] = X[category, idx_1].reshape(w, h, d)\n",
    "        idx_2 = np.random.randint(0, n_examples)\n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category\n",
    "        else:\n",
    "            #add a random number to the category modulo n_classes to ensure 2nd image has different category\n",
    "            category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "        pairs[1][i, :, :, :] = X[category_2,idx_2].reshape(w, h, d)\n",
    "    return pairs, targets\n",
    "\n",
    "def batch_generator(batch_size, X):\n",
    "    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size, X)\n",
    "        yield (pairs, targets)\n",
    "\n",
    "def train(model, X_train, batch_size=64, steps_per_epoch=100, epochs=10):\n",
    "    model.fit_generator(batch_generator(batch_size, X_train), steps_per_epoch=steps_per_epoch, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ip6AEIS6cm9V"
   },
   "outputs": [],
   "source": [
    "def get_batch_test(batch_size, X):\n",
    "    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = np.random.choice(n_classes, size=(batch_size,), replace=False)\n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "    pairs = [np.zeros((batch_size, h, w, d)) for i in range(2)]\n",
    "    # initialize vector for the targets, and make one half of it '1's, so 2nd half of batch has same class\n",
    "    targets = np.zeros((batch_size,))\n",
    "    targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = np.random.randint(0, n_examples)\n",
    "        pairs[0][i, :, :, :] = X[category, idx_1].reshape(w, h, d)\n",
    "        idx_2 = np.random.randint(0, n_examples)\n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category\n",
    "        else:\n",
    "            #add a random number to the category modulo n_classes to ensure 2nd image has different category\n",
    "            category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "        pairs[1][i, :, :, :] = X[category_2,idx_2].reshape(w, h, d)\n",
    "    return pairs, targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "id": "uL-6azyKcqqj",
    "outputId": "dd2a0b61-9301-42cc-a733-1de6c0ea7a07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 500, 32, 32, 3)\n",
      "2 64\n",
      "(64, 32, 32, 3)\n",
      "(64, 32, 32, 3)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_samples.shape)\n",
    "\n",
    "t1, t2 = get_batch_test(64, x_train_samples)\n",
    "print(len(t1), len(t2))\n",
    "print(t1[0].shape)\n",
    "print(t1[1].shape)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QBNX9nXMvgi"
   },
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, X, c, language=None):\n",
    "    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    indices = np.random.randint(0, n_examples, size=(N,))\n",
    "    if language is not None:\n",
    "        low, high = c[language]\n",
    "        if N > high - low:\n",
    "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "        categories = np.random.choice(range(low,high), size=(N,), replace=False)\n",
    "    else:  # if no language specified just pick a bunch of random letters\n",
    "        categories = np.random.choice(range(n_classes), size=(N,), replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
    "    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, d)\n",
    "    support_set = X[categories, indices, :, :]\n",
    "    support_set[0, :, :] = X[true_category, ex2]\n",
    "    support_set = support_set.reshape(N, w, h, d)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image, support_set]\n",
    "    return pairs, targets\n",
    "\n",
    "def test_oneshot(model, X, c, N=20, k=250, language=None, verbose=True):\n",
    "    \"\"\"Test average N-way oneshot learning accuracy of a siamese neural net over k one-shot tasks.\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(k, N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N, X, c, language=language)\n",
    "        probs = model.predict(inputs)\n",
    "        print('---------------------------')\n",
    "        print(len(inputs))\n",
    "        print(inputs[0].shape)\n",
    "        print(len(targets))\n",
    "#         print(targets.shape)\n",
    "        print(targets)\n",
    "        print(len(probs))\n",
    "        \n",
    "        print(np.argmax(probs))\n",
    "        print(np.argmax(targets))\n",
    "        print('---------------------------')\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct += 1\n",
    "    percent_correct = (100.0*n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% accuracy for {}-way one-shot learning\".format(percent_correct, N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 41397
    },
    "colab_type": "code",
    "id": "WwQPPcsHMvgn",
    "outputId": "b77ebb90-d478-4903-917e-bbf3dee18615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "7\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "4\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "0\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "9\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "9\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "10\n",
      "13\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "16\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "7\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "20\n",
      "0\n",
      "17\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "2\n",
      "6\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "20\n",
      "4\n",
      "17\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "11\n",
      "8\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "7\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "13\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "3\n",
      "7\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "12\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "11\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "2\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "12\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "9\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "19\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "20\n",
      "1\n",
      "19\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "13\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "19\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "11\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "14\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "7\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "20\n",
      "14\n",
      "17\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "11\n",
      "12\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "8\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "3\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "8\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "9\n",
      "13\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "13\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "20\n",
      "4\n",
      "19\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "13\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "7\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "6\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "9\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "11\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "10\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "3\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "3\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "14\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "6\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "10\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "4\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "10\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "8\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "10\n",
      "7\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "1\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "11\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "8\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "9\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "12\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "14\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "19\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "12\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "20\n",
      "14\n",
      "19\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "2\n",
      "13\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "6\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "15\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "6\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "4\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "6\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "9\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "20\n",
      "16\n",
      "19\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "8\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "19\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "0\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "20\n",
      "6\n",
      "17\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "3\n",
      "6\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "11\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "2\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "2\n",
      "4\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "3\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "11\n",
      "4\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "15\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "0\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "19\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "1\n",
      "0\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "12\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "18\n",
      "8\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "12\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "12\n",
      "12\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "12\n",
      "12\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "13\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "18\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "4\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "19\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "19\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "3\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "8\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "2\n",
      "12\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "10\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "3\n",
      "14\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "3\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "15\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "11\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "4\n",
      "7\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "12\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "7\n",
      "0\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "13\n",
      "8\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "7\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "12\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "20\n",
      "8\n",
      "19\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "10\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "1\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "7\n",
      "7\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "18\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "2\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "20\n",
      "15\n",
      "17\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "13\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "6\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "6\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "19\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "9\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "0\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "0\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "14\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "1\n",
      "13\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "20\n",
      "13\n",
      "15\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "2\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "7\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "2\n",
      "7\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "20\n",
      "7\n",
      "15\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "20\n",
      "7\n",
      "17\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "12\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "6\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "9\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "16\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "18\n",
      "7\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "11\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "14\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "4\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "12\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "20\n",
      "12\n",
      "17\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "9\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "1\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "7\n",
      "4\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "12\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "20\n",
      "14\n",
      "19\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "10\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "10\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "10\n",
      "13\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "20\n",
      "9\n",
      "17\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "4\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "0\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "17\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "0\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "11\n",
      "12\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "9\n",
      "14\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "13\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "7\n",
      "7\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "4\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "20\n",
      "8\n",
      "17\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "12\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "18\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "9\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "12\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "6\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "3\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "20\n",
      "15\n",
      "19\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "15\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "6\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "0\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "13\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "7\n",
      "12\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "19\n",
      "13\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "12\n",
      "9\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "10\n",
      "7\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "6\n",
      "2\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "20\n",
      "8\n",
      "18\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "20\n",
      "9\n",
      "19\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "3\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "6\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "8\n",
      "7\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "11\n",
      "0\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "10\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "20\n",
      "0\n",
      "19\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "1\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "10\n",
      "12\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "2\n",
      "14\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "5\n",
      "11\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "15\n",
      "14\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "3\n",
      "0\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "20\n",
      "17\n",
      "16\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "13\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "7\n",
      "12\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "20\n",
      "14\n",
      "5\n",
      "---------------------------\n",
      "---------------------------\n",
      "2\n",
      "(20, 32, 32, 3)\n",
      "20\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "20\n",
      "18\n",
      "15\n",
      "---------------------------\n",
      "Got an average of 9.6% accuracy for 20-way one-shot learning\n",
      "New best one-shot accuracy, saving model ...\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "\n",
    "# Siamese training\n",
    "# train(siamese_net, x_train_samples)\n",
    "# siamese_net.summary()\n",
    "# 20 way-one shot learning task\n",
    "test_acc = test_oneshot(siamese_net, x_test_shape, y_test)\n",
    "if test_acc >= best_acc:\n",
    "    print(\"New best one-shot accuracy, saving model ...\")\n",
    "#     siamese_net.save(os.path.join(\"models\", \"siamese_omniglot.h5\"))\n",
    "    best_acc = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "yzG42MltGnPk",
    "outputId": "327a6904-6035-4832-cfef-0d955021f276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 32, 32, 3)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test_shape.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COiAqXWDAgCe"
   },
   "source": [
    "***\n",
    "\n",
    "**b)** Compare the performance of your Siamese network for Cifar-100 to the Siamese network from Practical 4 for Omniglot. Name three fundamental differences between the Cifar-100 and Omniglot datasets. How do these differences influence the difference in one-shot accuracy?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IIHkoQ0PBWuB"
   },
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWpFF_5-Bf4B"
   },
   "source": [
    "***\n",
    "\n",
    "### Task 1.2: One-shot learning with neural codes\n",
    "**a)**\n",
    "* Train a CNN classifier on the first 80 classes of Cifar-100. Make sure it achieves at least 40% classification accuracy on those 80 classes (use the test set to validate this accuracy).\n",
    "* Then use neural codes from one of the later hidden layers of the CNN with L2-distance to evaluate one-shot learning accuracy for the remaining 20 classes of Cifar-100 with 250 random tasks. I.e. for a given one-shot task, obtain neural codes for the test image as well as the support set. Then pick the image from the support set that is closest (in L2-distance) to the test image as your one-shot prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "7n9sICFTMvgu",
    "outputId": "b185c534-bb1a-4527-deee-b6d5b3da1d2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3)\n",
      "(8000, 32, 32, 3)\n",
      "(40000, 80)\n",
      "(8000, 80)\n",
      "Class label: elephant\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_training_CNN = x_train_sort[:40000]\n",
    "y_training = y_train_sort[:40000]\n",
    "y_training_CNN = to_categorical(y_training, 80)\n",
    "\n",
    "x_test_CNN = x_test_sort[:8000]\n",
    "y_test = y_test_sort[:8000]\n",
    "y_test_CNN = to_categorical(y_test, 80)\n",
    "\n",
    "print(x_training_CNN.shape)\n",
    "print(x_test_CNN.shape)\n",
    "print(y_training_CNN.shape)\n",
    "print(y_test_CNN.shape)\n",
    "\n",
    "example_id = 15900  # pick any integer from 0 to 49999 to visualize a training example\n",
    "example = x_training_CNN[example_id]\n",
    "label = y_train_sort[example_id]\n",
    "print(\"Class label:\", class_labels[label])\n",
    "plt.imshow(example)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660
    },
    "colab_type": "code",
    "id": "qU7rNa157v6r",
    "outputId": "bb866ff1-922c-474c-8b81-c6dff159211d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 10, 10, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 5, 5, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 512)               1638912   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 80)                41040     \n",
      "=================================================================\n",
      "Total params: 1,940,880\n",
      "Trainable params: 1,940,496\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# === add code here ===\n",
    "input_shape = (32, 32, 3)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu', input_shape = input_shape))\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Conv2D(128,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(128,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,name = 'hidden_layer', activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(80,activation = 'softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adadelta\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "nNh4qrV1Mvgz",
    "outputId": "2c709dd8-0d15-4686-ae0f-03ea44eabec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3)\n",
      "(40000, 80)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 40000 samples, validate on 8000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 9s 215us/step - loss: 4.0651 - acc: 0.1157 - val_loss: 3.5660 - val_acc: 0.1666\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 8s 190us/step - loss: 3.2243 - acc: 0.2158 - val_loss: 2.9084 - val_acc: 0.2838\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 8s 190us/step - loss: 2.8745 - acc: 0.2819 - val_loss: 2.6780 - val_acc: 0.3295\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 8s 191us/step - loss: 2.6157 - acc: 0.3319 - val_loss: 2.9830 - val_acc: 0.2741\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 8s 197us/step - loss: 2.4257 - acc: 0.3708 - val_loss: 2.4735 - val_acc: 0.3776\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 8s 195us/step - loss: 2.2756 - acc: 0.4031 - val_loss: 2.3604 - val_acc: 0.3890\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 8s 193us/step - loss: 2.1722 - acc: 0.4272 - val_loss: 2.1321 - val_acc: 0.4426\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 8s 193us/step - loss: 2.0701 - acc: 0.4481 - val_loss: 2.1221 - val_acc: 0.4427\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 8s 193us/step - loss: 2.0007 - acc: 0.4644 - val_loss: 2.0812 - val_acc: 0.4554\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 8s 199us/step - loss: 1.9267 - acc: 0.4792 - val_loss: 1.9960 - val_acc: 0.4707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6520d3dd30>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "print(x_training_CNN.shape)\n",
    "print(y_training_CNN.shape)\n",
    "model.fit(x_training_CNN, y_training_CNN,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data = (x_test_CNN, y_test_CNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "SRw5mU2MOplR",
    "outputId": "c4e0c50f-051d-46fd-ebc9-98eb4e21c6a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 94us/step\n",
      "Test loss: 1.996033104479313\n",
      "Test accuracy: 0.47075\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test_CNN, y_test_CNN, verbose=1)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "colab_type": "code",
    "id": "r-trorOE3ArK",
    "outputId": "c637a1c8-afd7-4bab-e738-5bdd3d3700df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5_input (InputLayer)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 10, 10, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 5, 5, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 512)               1638912   \n",
      "=================================================================\n",
      "Total params: 1,899,840\n",
      "Trainable params: 1,899,456\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neural_codes_model = Model(inputs = model.input, outputs = model.get_layer(\"hidden_layer\").output)\n",
    "neural_codes_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ONjOoLqBE7s"
   },
   "outputs": [],
   "source": [
    "def make_oneshot_task2(N, X):\n",
    "    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    indices = np.random.randint(0, n_examples, size=(N,))\n",
    "    categories = np.random.choice(range(n_classes), size=(N,), replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
    "    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, d)\n",
    "    support_set = X[categories, indices, :, :]\n",
    "    support_set[0, :, :] = X[true_category, ex2]\n",
    "    support_set = support_set.reshape(N, w, h, d)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    #pairs = [test_image, support_set]\n",
    "    return test_image, support_set, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GW7i0iBNIQFl"
   },
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, X, c, language=None):\n",
    "    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    indices = np.random.randint(0, n_examples, size=(N,))\n",
    "    if language is not None:\n",
    "        low, high = c[language]\n",
    "        if N > high - low:\n",
    "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "        categories = np.random.choice(range(low,high), size=(N,), replace=False)\n",
    "    else:  # if no language specified just pick a bunch of random letters\n",
    "        categories = np.random.choice(range(n_classes), size=(N,), replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
    "    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, d)\n",
    "    support_set = X[categories, indices, :, :]\n",
    "    support_set[0, :, :] = X[true_category, ex2]\n",
    "    support_set = support_set.reshape(N, w, h, d)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image, support_set]\n",
    "    return pairs, targets\n",
    "\n",
    "def test_oneshot(model, X, c, N=20, k=250, language=None, verbose=True):\n",
    "    \"\"\"Test average N-way oneshot learning accuracy of a siamese neural net over k one-shot tasks.\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(k, N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N, X, c, language=language)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct += 1\n",
    "    percent_correct = (100.0*n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% accuracy for {}-way one-shot learning\".format(percent_correct, N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "PgE2EVhABYop",
    "outputId": "868c9ee8-63e9-4aea-d9cd-5a6fec7ce800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got an average of 19.6% accuracy for 20-way one-shot learning\n"
     ]
    }
   ],
   "source": [
    "# One shot learning\n",
    "N=20\n",
    "n_correct = 0\n",
    "k=250\n",
    "\n",
    "for _ in range(0,250):\n",
    "    test_image, support_set, targets = make_oneshot_task2(N,x_train_rem_samples)\n",
    "    test_image_pred = neural_codes_model.predict(test_image)\n",
    "    support_set_pred = neural_codes_model.predict(support_set)\n",
    "    distances = []\n",
    "\n",
    "    for i in range(0,20):\n",
    "        dist = distance.cityblock(test_image_pred[i],support_set_pred[i])\n",
    "        distances.append(dist)\n",
    "    if np.argmin(distances) == np.argmax(targets):\n",
    "        n_correct += 1\n",
    "accuracy = (100.0*n_correct / k)\n",
    "print(\"Got an average of {}% accuracy for {}-way one-shot learning\".format(accuracy, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M1BDzdPAz26B"
   },
   "source": [
    "***\n",
    "\n",
    "**b)** Briefly motivate your CNN architecture, and discuss the difference in one-shot accuracy between the Siamese network approach and the CNN neural codes approach.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRpVm956FR8P"
   },
   "source": [
    "*=== write your answer here ===*\n",
    "\n",
    "Initially the feature for convolution layer was 32, 64 and 128. but the accuracy was not achieved as there isnt enough features. Dropout of 0.2 to 0.5 was tried with the same architecture. Then we came to conclusion that the features were not enough for it to train on, so decided to choose more features and thus started with 64 and 128 in convolution layer. The maxpoolig was added and dropouts were chosen to be 0.35 to avoid overfitting and drop the similar neurons to reduce the density of the network. BatchNormalization was done in between these steps to normalize the output from maxpooling before dropping out and also to increase the speed of training. The last layer is fully connected dense layer with 100 output classes and the activation is softmax as we need to choose from the classification. The optimizer adadelta continuously learns and is most suitable for classification tasks. Trying with 10 epoch we achieved an accuracy of 40.23% whereas with 30 epochs and having 2 dense layers (one with 512 neurons and output with 100 neurons) we achieved an accuracy 41.84%. This result is due to extensive training of the model with more iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-gkaM1tCThc"
   },
   "source": [
    "***\n",
    "## Question 2: Triplet networks & one-shot learning (10pt)\n",
    "\n",
    "### Task 2.1: Train a triplet network\n",
    "**a)**\n",
    "* Train a triplet network on the first 80 classes of (the training set of) Cifar-100.\n",
    " \n",
    "* Make sure the network achieves a smaller loss than the margin and the network does not collapse all representations to zero vectors. *HINT: If you experience problems to achieve this goal, it might be helpful to tinker the learning rate.*\n",
    "\n",
    "* You are provided with a working example of triplet loss implementation for Keras below. You may directly use it.\n",
    "\n",
    "You may ignore the test set of Cifar-100 for this question as well. It suffices to use only the training set and split this, using the first 80 classes for training and the remaining 20 classes for one-shot testing.\n",
    "\n",
    "```python\n",
    "# Notice that ground truth variable is not used for loss calculation. It is used as a function argument to by-pass some Keras functionality. This is because the network structure already implies the ground truth for the anchor image with the \"positive\" image.\n",
    "import tensorflow as tf\n",
    "def triplet_loss(ground_truth, network_output):\n",
    "\n",
    "    anchor, positive, negative = tf.split(network_output, num_or_size_splits=3, axis=1)        \n",
    "    \n",
    "    for embedding in [anchor, positive, negative]:\n",
    "        embedding = tf.math.l2_normalize(embedding)\n",
    "\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=1)\n",
    "    \n",
    "    margin = # define your margin\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), margin)\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), axis=0)\n",
    "\n",
    "    return loss\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1d2hbyOubTjk"
   },
   "outputs": [],
   "source": [
    "# Triplet Loss function, created to bypass some Keras loss implementation requirements to work with current use case\n",
    "\n",
    "import tensorflow as tf\n",
    "def triplet_loss(ground_truth, network_output):\n",
    "\n",
    "    anchor, positive, negative = tf.split(network_output, num_or_size_splits=3, axis=1)        \n",
    "\n",
    "    for embedding in [anchor, positive, negative]:\n",
    "        embedding = tf.math.l2_normalize(embedding)\n",
    "\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=1)\n",
    "\n",
    "    margin = 0.5# define your margin\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), margin)\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), axis=0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# in_dims = [32,32,3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0th6D7vVZz5n"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fpWVI6fjSZbq"
   },
   "outputs": [],
   "source": [
    "def get_triplets(batch_size, X):\n",
    "    \"\"\"Create batch of n triplets, half same class, half different class\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "#     print(n_classes, n_examples, w,h,d)\n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = np.random.choice(n_classes, size=(batch_size,), replace=False)\n",
    "#     print(\"Categories:\")\n",
    "#     print(categories)\n",
    "    # initialize 3 empty arrays for the input image batch\n",
    "    triplets = [np.zeros((batch_size, h, w, d)) for i in range(3)]\n",
    "    # initialize vector for the targets, all zeroes coz there's both a positive and a negative, and loss doesn't use labels, which is super fascinating, \n",
    "    # in a weird way, since the architecture design is unconventional\n",
    "    targets = np.zeros((batch_size,))\n",
    "#     targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = np.random.randint(0, n_examples)\n",
    "        triplets[0][i, :, :, :] = X[category, idx_1].reshape(w, h, d)\n",
    "        idx_2 = np.random.randint(0, n_examples)\n",
    "        triplets[1][i, :, :, :] = X[category, idx_2].reshape(w, h, d)\n",
    "        idx_3 = np.random.randint(0, n_examples)\n",
    "        category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "        triplets[2][i, :, :, :] = X[category_2, idx_3].reshape(w, h, d)\n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "    return triplets, targets\n",
    "\n",
    "\n",
    "\n",
    "def triplet_generator(batch_size, X):\n",
    "    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "    while True:\n",
    "        triplets, targets = get_triplets(batch_size, X)\n",
    "        yield (triplets, targets)\n",
    "\n",
    "def train_triplet_network(model, X_train, batch_size=64, steps_per_epoch=100, epochs=40):\n",
    "    model.fit_generator(triplet_generator(batch_size, X_train), steps_per_epoch=steps_per_epoch, epochs=epochs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "0kkBc4ES1xeV",
    "outputId": "1480bf3b-f55b-4927-ed5f-ff85325cf6f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 500, 32, 32, 3)\n",
      "(40000,)\n",
      "(20, 500, 32, 32, 3)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Data for triplet\n",
    "\n",
    "x_training_triplet = x_train_samples\n",
    "y_training_triplet = y_train_samples\n",
    "\n",
    "print(x_training_triplet.shape)\n",
    "print(y_training_triplet.shape)\n",
    "\n",
    "x_test_triplet = x_train_rem_samples\n",
    "y_test_triplet = y_train_rem_samples\n",
    "\n",
    "print(x_test_triplet.shape)\n",
    "print(y_test_triplet.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQCmn5kTa4kd"
   },
   "outputs": [],
   "source": [
    "def get_triplet_test(batch_size, X):\n",
    "    \"\"\"Create batch of n triplets, half same class, half different class\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    print(n_classes, n_examples, w,h,d)\n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = np.random.choice(n_classes, size=(batch_size,), replace=False)\n",
    "    print(\"Categories:\")\n",
    "    print(categories)\n",
    "    # initialize 3 empty arrays for the input image batch\n",
    "    triplets = [np.zeros((batch_size, h, w, d)) for i in range(3)]\n",
    "    # initialize vector for the targets, all zeroes coz there's both a positive and a negative, and loss doesn't use labels, which is super fascinating, \n",
    "    # in a weird way, since the architecture design is unconventional\n",
    "    targets = np.zeros((batch_size,))\n",
    "#     targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = np.random.randint(0, n_examples)\n",
    "        triplets[0][i, :, :, :] = X[category, idx_1].reshape(w, h, d)\n",
    "        idx_2 = np.random.randint(0, n_examples)\n",
    "        triplets[1][i, :, :, :] = X[category, idx_2].reshape(w, h, d)\n",
    "        idx_3 = np.random.randint(0, n_examples)\n",
    "        category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "        triplets[2][i, :, :, :] = X[category_2, idx_3].reshape(w, h, d)\n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "    return triplets, targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "fQRi7qIsa4kw",
    "outputId": "9ff12004-22e1-4e4f-ce1d-89fd8b97f659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 500, 32, 32, 3)\n",
      "80 500 32 32 3\n",
      "Categories:\n",
      "[28 56 55 66 19 11 43  1  0 26 61 27 16 68 72 29 51 20 40 71 54 50  8 58\n",
      " 35 74 24 79  7  6 22 57 75 48 49 12 10 18 60 64  9 33 44 21 38 65 62 78\n",
      " 13  5 45 34 67 69 41 59 53 70 14 76 52 77  3 63]\n",
      "3 64\n",
      "(64, 32, 32, 3)\n",
      "(64, 32, 32, 3)\n",
      "(64, 32, 32, 3)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_training_triplet.shape)\n",
    "\n",
    "t1, t2 = get_triplets(64, x_training_triplet)\n",
    "print(len(t1), len(t2))\n",
    "print(t1[0].shape)\n",
    "print(t1[1].shape)\n",
    "print(t1[2].shape)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1486
    },
    "colab_type": "code",
    "id": "fRmiRbKplgzG",
    "outputId": "98364f97-7d9a-4a57-c6d6-b35f486a0e1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 21.3362\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 20.6031\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 19.5821\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 18.2802\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 15.8923\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 15.8959\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 14.9228\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 13.4947\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 12.4719\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 12.0640\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 11.3228\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 10.1448\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 10.3114\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 9.0605\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 8.4723\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 8.2491\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 7.5551\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 7.1705\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 6.4430\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 5.8347\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 5.5405\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 5.2186\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 4.6277\n",
      "Epoch 24/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 4.4518\n",
      "Epoch 25/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 4.0695\n",
      "Epoch 26/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 3.7559\n",
      "Epoch 27/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 3.4806\n",
      "Epoch 28/40\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 3.1395\n",
      "Epoch 29/40\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 2.9640\n",
      "Epoch 30/40\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 2.6350\n",
      "Epoch 31/40\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 2.5210\n",
      "Epoch 32/40\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 2.3217\n",
      "Epoch 33/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 2.1196\n",
      "Epoch 34/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.9634\n",
      "Epoch 35/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.7850\n",
      "Epoch 36/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.6433\n",
      "Epoch 37/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.5269\n",
      "Epoch 38/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.4086\n",
      "Epoch 39/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.3163\n",
      "Epoch 40/40\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 1.1889\n"
     ]
    }
   ],
   "source": [
    "train_triplet_network(triplet_net, x_training_triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "TFuK7hD4YohY",
    "outputId": "10808e4e-adde-400c-fd1e-9a88b154391c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3)\n",
      "(40000, 80)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_training_CNN.shape)\n",
    "\n",
    "print(y_training_CNN.shape)\n",
    "print(y_training_CNN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1082
    },
    "colab_type": "code",
    "id": "GwrNzriE-vsZ",
    "outputId": "63b5f9cd-aaca-498e-e089-3261276ce8ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_45 (Conv2D)           (None, 30, 30, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 2, 2, 256)         295168    \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 1024)              0         \n",
      "=================================================================\n",
      "Total params: 523,776\n",
      "Trainable params: 521,088\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_30 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_31 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_32 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 1024)         523776      input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3072)         0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "                                                                 sequential_12[3][0]              \n",
      "==================================================================================================\n",
      "Total params: 523,776\n",
      "Trainable params: 521,088\n",
      "Non-trainable params: 2,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "anchor_input = Input(input_shape)\n",
    "positive_input = Input(input_shape)\n",
    "negative_input = Input(input_shape)\n",
    "\n",
    "# build convnet to use in each siamese 'leg'\n",
    "convnet_triplet = Sequential()\n",
    "\n",
    "convnet_triplet.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "convnet_triplet.add(MaxPooling2D())\n",
    "convnet_triplet.add(BatchNormalization())\n",
    "convnet_triplet.add(Dropout(0.25))\n",
    "\n",
    "convnet_triplet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet_triplet.add(MaxPooling2D())\n",
    "convnet_triplet.add(BatchNormalization())\n",
    "convnet_triplet.add(Dropout(0.25))\n",
    "\n",
    "convnet_triplet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet_triplet.add(BatchNormalization())\n",
    "convnet_triplet.add(Dropout(0.25))\n",
    "\n",
    "convnet_triplet.add(Conv2D(256, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet_triplet.add(Flatten())\n",
    "convnet_triplet.add(BatchNormalization())\n",
    "convnet_triplet.add(Dropout(0.25))\n",
    "\n",
    "# convnet_triplet.add(Dense(6561, activation=\"sigmoid\", kernel_regularizer=l2(1e-3)))\n",
    "convnet_triplet.summary()\n",
    "\n",
    "# encode each of the two inputs into a vector with the convnet\n",
    "encoded_anchor = convnet_triplet(anchor_input)\n",
    "encoded_positive = convnet_triplet(positive_input)\n",
    "encoded_negative = convnet_triplet(negative_input)\n",
    "\n",
    "# merge two encoded inputs with the L1 distance between them, and connect to prediction output layer\n",
    "# L2_distance = lambda x: K.abs(x[0]-x[1])\n",
    "\n",
    "# all_three = Lambda(L2_distance)([encoded_anchor, encoded_positive, encoded_negative])\n",
    "# prediction_t = Dense(1, activation='sigmoid')(all_three)\n",
    "\n",
    "merged_vector = Concatenate(axis=-1)([encoded_anchor, encoded_positive, encoded_negative])\n",
    "# prediction_t = Dense(6561, activation=\"sigmoid\", kernel_regularizer=l2(1e-3))(merged_vector)\n",
    "triplet_net = Model(inputs=[anchor_input, positive_input ,negative_input], outputs=merged_vector)\n",
    "# triplet_net = Model(inputs=[anchor_input, positive_input ,negative_input])\n",
    "# triplet_net.add(prediction_t)\n",
    "\n",
    "\n",
    "triplet_net.compile(loss=triplet_loss, optimizer=\"adam\")\n",
    "\n",
    "triplet_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RmN5FNgzpGd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U7xaiQhWzlPF"
   },
   "outputs": [],
   "source": [
    "# Model layers of Triplet network\n",
    "\n",
    "baseModel = Sequential()\n",
    "baseModel.add(Conv2D(128,(7,7),padding='same',input_shape=(in_dims[0],in_dims[1],in_dims[2],),activation='relu',name='conv1'))\n",
    "baseModel.add(MaxPooling2D((2,2),(2,2),padding='same',name='pool1'))\n",
    "baseModel.add(Conv2D(256,(5,5),padding='same',activation='relu',name='conv2'))\n",
    "baseModel.add(MaxPooling2D((2,2),(2,2),padding='same',name='pool2'))\n",
    "baseModel.add(Flatten(name='flatten'))\n",
    "baseModel.add(Dense(4,name='embeddings'))\n",
    "# model.add(Dense(600))\n",
    "\n",
    "anchor_input = Input((32,32,3, ), name='anchor_input')\n",
    "positive_input = Input((32,32,3, ), name='positive_input')\n",
    "negative_input = Input((32,32,3, ), name='negative_input')\n",
    "\n",
    "encoded_anchor = baseModel(anchor_input)\n",
    "encoded_positive = baseModel(positive_input)\n",
    "encoded_negative = baseModel(negative_input)\n",
    "\n",
    "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
    "\n",
    "model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
    "model.compile(loss=triplet_loss, optimizer=adam_optim)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHGJp45AR1qm"
   },
   "source": [
    "***\n",
    "\n",
    "### Task 2.2: One-shot learning with triplet neural codes\n",
    "**a)**\n",
    "* Use neural codes from the triplet network with L2-distance to evaluate one-shot learning accuracy for the remaining 20 classes of Cifar-100 with 250 random tasks. I.e. for a given one-shot task, obtain neural codes for the test image as well as the support set. Then pick the image from the support set that is closest (in L2-distance) to the test image as your one-shot prediction.\n",
    "* Explicitly state the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7w6o8xIXUADN"
   },
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, X, c, language=None):\n",
    "    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    indices = np.random.randint(0, n_examples, size=(N,))\n",
    "    if language is not None:\n",
    "        low, high = c[language]\n",
    "        if N > high - low:\n",
    "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "        categories = np.random.choice(range(low,high), size=(N,), replace=False)\n",
    "    else:  # if no language specified just pick a bunch of random letters\n",
    "        categories = np.random.choice(range(n_classes), size=(N,), replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
    "    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, d)\n",
    "    support_set = X[categories, indices, :, :]\n",
    "    support_set[0, :, :] = X[true_category, ex2]\n",
    "    support_set = support_set.reshape(N, w, h, d)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image, support_set]\n",
    "    return pairs, targets\n",
    "\n",
    "def test_oneshot(model, X, c, N=20, k=250, language=None, verbose=True):\n",
    "    \"\"\"Test average N-way oneshot learning accuracy of a siamese neural net over k one-shot tasks.\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(k, N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N, X, c, language=language)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct += 1\n",
    "    percent_correct = (100.0*n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% accuracy for {}-way one-shot learning\".format(percent_correct, N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iu_6yEO1uaGu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCcmbz0UU7mR"
   },
   "source": [
    "***\n",
    "## Question 3: Performance comparison (3pt)\n",
    "\n",
    "\n",
    "**a)** What accuracy would random guessing achieve (on average) on this dataset? Motivate your answer briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BKGDydqsVVX1"
   },
   "source": [
    "*=== write your answer here ===*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KLXRv-eV04Q"
   },
   "source": [
    "**b)** Discuss and compare the performances of networks in tasks 1.1, 1.2 and 2.2. Briefly motivate and explain which task would be expected the highest accuracy. Explain the reasons of the accuracy difference if there are any. If there is almost no difference accuracy, explain the reason for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "71kTHFBkcjp8"
   },
   "source": [
    "*=== write your answer here ===*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pn1Q0zG1Mvg_"
   },
   "source": [
    "***\n",
    "## Question 4: Peer review (0pt)\n",
    "\n",
    "Finally, each group member must write a single paragraph outlining their opinion on the work distribution within the group. Did every group member contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vL2NQVFxMvg_"
   },
   "source": [
    "*=== write your answer here ===*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_2_1_18_05_Oneshort.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
