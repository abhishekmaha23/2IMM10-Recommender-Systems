{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uj4T8PEHGbMF"
   },
   "source": [
    "# Assignment 2\n",
    "## Question 1: Siamese networks & one-shot learning (7pt)\n",
    "The Cifar-100 dataset is similar to the Cifar-10 dataset. It also consists of 60,000 32x32 RGB images, but they are distributed over 100 classes instead of 10. Thus, each class has much fewer examples, only 500 training images and 100 testing images per class. For more info about the dataset, see https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "\n",
    "*HINT: Import the Cifar-100 dataset directly from Keras, no need to download it from the website. Use* `label_mode=\"fine\"`\n",
    "\n",
    "### Task 1.1: Siamese network\n",
    "**a)**\n",
    "* Train a Siamese Network on the first 80 classes of (the training set of) Cifar-100, i.e. let the network predict the probability that two input images are from the same class. Use 1 as a target for pairs of images from the same class (positive pairs), and 0 for pairs of images from different classes (negative pairs). Randomly select image pairs from Cifar-100, but make sure you train on as many positive pairs as negative pairs.\n",
    "\n",
    "* Evaluate the performance of the network on 20-way one-shot learning tasks. Do this by generating 250 random tasks and obtain the average accuracy for each evaluation round. Use the remaining 20 classes that were not used for training. The model should perform better than random guessing.\n",
    "\n",
    "For this question you may ignore the test set of Cifar-100; it suffices to use only the training set and split this, using the first 80 classes for training and the remaining 20 classes for one-shot testing.\n",
    "\n",
    "*HINT: First sort the data by their labels (see e.g.* `numpy.argsort()`*), then reshape the data to a shape of* `(n_classes, n_examples, width, height, depth)`*, similar to the Omniglot data in Practical 4. It is then easier to split the data by class, and to sample positive and negative images pairs for training the Siamese network.*\n",
    "\n",
    "*NOTE: do not expect the one-shot accuracy for Cifar-100 to be similar to that accuracy for Omniglot; a lower accuracy can be expected. However, accuracy higher than random guess is certainly achievable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B5XlMo9yMvf5",
    "outputId": "3e108bc5-f12c-4bb8-a9f9-e4a04017a40c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20181592\\AppData\\Local\\Continuum\\anaconda3\\envs\\py3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar100\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.spatial import distance\n",
    "from keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MaoAaEv17v6k"
   },
   "outputs": [],
   "source": [
    "# === add code here ===\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=\"fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iiaEnHNMMvgA",
    "outputId": "c10e86a8-6c4d-426f-958b-5f9d6eddb74e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "2JdsjUSzMvgD",
    "outputId": "efef02ea-2824-4a38-ae5c-288749acf264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mgoiy9tqMvgF"
   },
   "outputs": [],
   "source": [
    "class_labels = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
    "    'worm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "6aVrASEXMvgH",
    "outputId": "80ccb7e1-8a8f-43ae-cde2-d4c596e333ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: lion\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_id = 14130  # pick any integer from 0 to 49999 to visualize a training example\n",
    "example = x_train[example_id]\n",
    "label = y_train[example_id]\n",
    "print(\"Class label:\", class_labels[label])\n",
    "plt.imshow(example)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "coyB9jvFMvgJ",
    "outputId": "76b65b10-82e4-47a1-b2db-b31deb207053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000,)\n",
      "(10000, 32, 32, 3)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "img_rows,img_cols,chns = 32, 32, 3\n",
    "n_classes = 100\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, chns)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, chns)\n",
    "input_shape = (img_rows, img_cols, chns)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gNBIWXg9MvgM",
    "outputId": "28b9e148-80fb-47a1-ffef-f42fb200f569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42957  2126 43210 ... 42095 26322 21122]\n",
      "[9221 1027 2426 ... 8387 9860 9153]\n"
     ]
    }
   ],
   "source": [
    "indexes_train = y_train.argsort()\n",
    "indexes_test = y_test.argsort()\n",
    "print(indexes_train)\n",
    "print(indexes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "_Y6mh6ZYMvgP",
    "outputId": "d335b46b-d34a-4c6a-e93a-f6558184ae02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000,)\n",
      "(10000, 32, 32, 3)\n",
      "(10000,)\n",
      "Class label: spider\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_sort = x_train[indexes_train]\n",
    "y_train_sort = y_train[indexes_train]\n",
    "\n",
    "x_test_sort = x_test[indexes_test]\n",
    "y_test_sort = y_test[indexes_test]\n",
    "\n",
    "print(x_train_sort.shape)\n",
    "print(y_train_sort.shape)\n",
    "print(x_test_sort.shape)\n",
    "print(y_test_sort.shape)\n",
    "\n",
    "example_id = 39999  # pick any integer from 0 to 49999 to visualize a training example\n",
    "example = x_train_sort[example_id]\n",
    "label = y_train_sort[example_id]\n",
    "print(\"Class label:\", class_labels[label])\n",
    "plt.imshow(example)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCsrdPYcMvgU"
   },
   "outputs": [],
   "source": [
    "def group(a, b):\n",
    "    # Get argsort indices, to be used to sort a and b in the next steps\n",
    "    sidx = b.argsort(kind='mergesort')\n",
    "    a_sorted = a[sidx]\n",
    "    b_sorted = b[sidx]\n",
    "\n",
    "    # Get the group limit indices (start, stop of groups)\n",
    "    cut_idx = np.flatnonzero(np.r_[True,b_sorted[1:] != b_sorted[:-1],True])\n",
    "\n",
    "    # Create cut indices for all unique IDs in b\n",
    "    n = b_sorted[-1]+2\n",
    "    cut_idxe = np.full(n, cut_idx[-1], dtype=int)\n",
    "\n",
    "    insert_idx = b_sorted[cut_idx[:-1]]\n",
    "    cut_idxe[insert_idx] = cut_idx[:-1]\n",
    "    cut_idxe = np.minimum.accumulate(cut_idxe[::-1])[::-1]\n",
    "\n",
    "    # Split input array with those start, stop ones\n",
    "    out = [a_sorted[i:j] for i,j in zip(cut_idxe[:-1],cut_idxe[1:])]\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Zf0wCf1nMvgW",
    "outputId": "ba80beaf-c55d-4a45-8f55-3bd6e8c16dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 500, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train_shape = group(x_train_sort, y_train_sort)\n",
    "x_test_shape = group(x_test, y_test)\n",
    "print(x_train_shape.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "B8u0pBHZMvgZ",
    "outputId": "dfe387e0-62aa-4356-c1ff-3efc992d888c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 500, 32, 32, 3)\n",
      "(40000,)\n",
      "(20, 500, 32, 32, 3)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train_samples = x_train_shape[:80]\n",
    "y_train_samples = y_train_sort[:40000]\n",
    "\n",
    "x_train_rem_samples = x_train_shape[80:]\n",
    "y_train_rem_samples = y_train_sort[40000:]\n",
    "\n",
    "print(x_train_samples.shape)\n",
    "print(y_train_samples.shape)\n",
    "print(x_train_rem_samples.shape)\n",
    "print(y_train_rem_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1163
    },
    "colab_type": "code",
    "id": "SE5d-Oq7Mvgb",
    "outputId": "6b6e6cbe-e9bb-4dd9-e2cd-0b9159dc0f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 256)         295168    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              4198400   \n",
      "=================================================================\n",
      "Total params: 4,722,176\n",
      "Trainable params: 4,719,488\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 4096)         4722176     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4096)         0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,726,273\n",
      "Trainable params: 4,723,585\n",
      "Non-trainable params: 2,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "\n",
    "# build convnet to use in each siamese 'leg'\n",
    "convnet = Sequential()\n",
    "\n",
    "convnet.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "\n",
    "convnet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(MaxPooling2D())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "\n",
    "convnet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "\n",
    "convnet.add(Conv2D(256, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(BatchNormalization())\n",
    "convnet.add(Dropout(0.25))\n",
    "\n",
    "convnet.add(Dense(4096, activation=\"sigmoid\", kernel_regularizer=l2(1e-3)))\n",
    "convnet.summary()\n",
    "\n",
    "# encode each of the two inputs into a vector with the convnet\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "\n",
    "# merge two encoded inputs with the L1 distance between them, and connect to prediction output layer\n",
    "L1_distance = lambda x: K.abs(x[0]-x[1])\n",
    "both = Lambda(L1_distance)([encoded_l, encoded_r])\n",
    "prediction = Dense(1, activation='sigmoid')(both)\n",
    "siamese_net = Model(inputs=[left_input,right_input], outputs=prediction)\n",
    "\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRpbGhM4Mvgd"
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size, X):\n",
    "    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = np.random.choice(n_classes, size=(batch_size,), replace=False)\n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "    pairs = [np.zeros((batch_size, h, w, d)) for i in range(2)]\n",
    "    # initialize vector for the targets, and make one half of it '1's, so 2nd half of batch has same class\n",
    "    targets = np.zeros((batch_size,))\n",
    "    targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = np.random.randint(0, n_examples)\n",
    "        pairs[0][i, :, :, :] = X[category, idx_1].reshape(w, h, d)\n",
    "        idx_2 = np.random.randint(0, n_examples)\n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category\n",
    "        else:\n",
    "            #add a random number to the category modulo n_classes to ensure 2nd image has different category\n",
    "            category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "        pairs[1][i, :, :, :] = X[category_2,idx_2].reshape(w, h, d)\n",
    "    return pairs, targets\n",
    "\n",
    "def batch_generator(batch_size, X):\n",
    "    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size, X)\n",
    "        yield (pairs, targets)\n",
    "\n",
    "def train(model, X_train, batch_size=64, steps_per_epoch=100, epochs=1):\n",
    "    model.fit_generator(batch_generator(batch_size, X_train), steps_per_epoch=steps_per_epoch, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QBNX9nXMvgi"
   },
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, X, c, language=None):\n",
    "    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    indices = np.random.randint(0, n_examples, size=(N,))\n",
    "    if language is not None:\n",
    "        low, high = c[language]\n",
    "        if N > high - low:\n",
    "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "        categories = np.random.choice(range(low,high), size=(N,), replace=False)\n",
    "    else:  # if no language specified just pick a bunch of random letters\n",
    "        categories = np.random.choice(range(n_classes), size=(N,), replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
    "    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, d)\n",
    "    support_set = X[categories, indices, :, :]\n",
    "    support_set[0, :, :] = X[true_category, ex2]\n",
    "    support_set = support_set.reshape(N, w, h, d)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image, support_set]\n",
    "    return pairs, targets\n",
    "\n",
    "def test_oneshot(model, X, c, N=20, k=250, language=None, verbose=True):\n",
    "    \"\"\"Test average N-way oneshot learning accuracy of a siamese neural net over k one-shot tasks.\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(k, N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N, X, c, language=language)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct += 1\n",
    "    percent_correct = (100.0*n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% accuracy for {}-way one-shot learning\".format(percent_correct, N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "WwQPPcsHMvgn",
    "outputId": "75d55103-ed6a-4e0a-a2c8-edf321b0f228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training loop 1 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.7282: 0s - los\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 8.0% accuracy for 20-way one-shot learning\n",
      "New best one-shot accuracy, saving model ...\n",
      "=== Training loop 2 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.7243\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 8.0% accuracy for 20-way one-shot learning\n",
      "New best one-shot accuracy, saving model ...\n",
      "=== Training loop 3 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.7199: 0s - lo\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.8% accuracy for 20-way one-shot learning\n",
      "New best one-shot accuracy, saving model ...\n",
      "=== Training loop 4 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 0.7224\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "New best one-shot accuracy, saving model ...\n",
      "=== Training loop 5 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.7089\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 6 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.7031\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 8.8% accuracy for 20-way one-shot learning\n",
      "=== Training loop 7 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 6s 65ms/step - loss: 0.7017: 0s - loss: 0.702\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 12.0% accuracy for 20-way one-shot learning\n",
      "=== Training loop 8 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.703 - 6s 64ms/step - loss: 0.7044\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.2% accuracy for 20-way one-shot learning\n",
      "New best one-shot accuracy, saving model ...\n",
      "=== Training loop 9 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.7100: 0s - \n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 10.4% accuracy for 20-way one-shot learning\n",
      "=== Training loop 10 ===\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 6s 64ms/step - loss: 0.7080\n",
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
      "New best one-shot accuracy, saving model ...\n"
     ]
    }
   ],
   "source": [
    "loops = 10\n",
    "best_acc = 0\n",
    "for i in range(loops):\n",
    "    print(\"=== Training loop {} ===\".format(i+1))\n",
    "    # Siamese training\n",
    "    train(siamese_net, x_train_samples)\n",
    "    # 20 way-one shot learning task\n",
    "    test_acc = test_oneshot(siamese_net, x_train_rem_samples, y_train_rem_samples)\n",
    "    if test_acc >= best_acc:\n",
    "        print(\"New best one-shot accuracy, saving model ...\")\n",
    "        siamese_net.save(os.path.join(\"models\", \"siamese_omniglot.h5\"))\n",
    "        best_acc = test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COiAqXWDAgCe"
   },
   "source": [
    "***\n",
    "\n",
    "**b)** Compare the performance of your Siamese network for Cifar-100 to the Siamese network from Practical 4 for Omniglot. Name three fundamental differences between the Cifar-100 and Omniglot datasets. How do these differences influence the difference in one-shot accuracy?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IIHkoQ0PBWuB"
   },
   "source": [
    "The performance of Siamese network for Omniglot from Practical 4 achieved an accuracy of 40% whereas the performance of Siamese network for Cifar-100 is around 16%. This difference is caused by:\n",
    "\n",
    "Omniglot dataset has 1643 classes (964 for training and 659 for testing) with 20 examples per class. Whereas Cifar-100 has 100 classes with 600 examples per class (500 for training and 100 for testing).\n",
    "\n",
    "Omniglot dataset composed of handwritten symbols. And due to its size of the image (105x105) we use kernel with 10x10 size and this helps in getting the features extracted (the images have clear edges so few features are enough to represent them). The 1st convolution layer returns 6464 features.\n",
    "Cifar-100 are natural images with shape 32x32x3 (RGB image). They have varying background and most of the image is pixelated thus having noise in it. So, this uses a kernel of size 3x3 so as to not lose the features on the edges and has 1792 features after the 1st convolution layer.\n",
    "\n",
    "Since Cifar-100 is natural and coloured image it is hard for siamese network to recognize the images in one-shot learning. Its representations are harder and deeper on same image. The same class of image can be represented with different background and in different angle too. So this contributes to various possibility of representing a class and makes it more difficult to identify with high accuracy\n",
    "Whereas Omniglot dataset is composed of gray scale well defined symbols. The class of a symbol can be represented only in particular angle with few deformations in some images that can be identified with higher accuracy than Cifar-100. Also there is no variation in background thus reducing the region of interest for one-shot learning.\n",
    "\n",
    "Due to the presence of these significant differences there is difference in their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWpFF_5-Bf4B"
   },
   "source": [
    "***\n",
    "\n",
    "### Task 1.2: One-shot learning with neural codes\n",
    "**a)**\n",
    "* Train a CNN classifier on the first 80 classes of Cifar-100. Make sure it achieves at least 40% classification accuracy on those 80 classes (use the test set to validate this accuracy).\n",
    "* Then use neural codes from one of the later hidden layers of the CNN with L2-distance to evaluate one-shot learning accuracy for the remaining 20 classes of Cifar-100 with 250 random tasks. I.e. for a given one-shot task, obtain neural codes for the test image as well as the support set. Then pick the image from the support set that is closest (in L2-distance) to the test image as your one-shot prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "7n9sICFTMvgu",
    "outputId": "0de154cc-2396-4d90-bdd8-3151706abc39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3)\n",
      "(8000, 32, 32, 3)\n",
      "(40000, 80)\n",
      "(8000, 80)\n",
      "Class label: elephant\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_training_CNN = x_train_sort[:40000]\n",
    "y_training = y_train_sort[:40000]\n",
    "y_training_CNN = to_categorical(y_training, 80)\n",
    "\n",
    "x_test_CNN = x_test_sort[:8000]\n",
    "y_test = y_test_sort[:8000]\n",
    "y_test_CNN = to_categorical(y_test, 80)\n",
    "\n",
    "print(x_training_CNN.shape)\n",
    "print(x_test_CNN.shape)\n",
    "print(y_training_CNN.shape)\n",
    "print(y_test_CNN.shape)\n",
    "\n",
    "example_id = 15900  # pick any integer from 0 to 49999 to visualize a training example\n",
    "example = x_training_CNN[example_id]\n",
    "label = y_train_sort[example_id]\n",
    "print(\"Class label:\", class_labels[label])\n",
    "plt.imshow(example)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "colab_type": "code",
    "id": "qU7rNa157v6r",
    "outputId": "39ebc08e-92e0-48ec-c378-753649df71c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 10, 10, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 5, 5, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 512)               1638912   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 80)                41040     \n",
      "=================================================================\n",
      "Total params: 1,940,880\n",
      "Trainable params: 1,940,496\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# === add code here ===\n",
    "input_shape = (32, 32, 3)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu', input_shape = input_shape))\n",
    "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Conv2D(128,(3,3),activation = 'relu'))\n",
    "model.add(Conv2D(128,(3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,name = 'hidden_layer', activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(80,activation = 'softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adadelta\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1143
    },
    "colab_type": "code",
    "id": "nNh4qrV1Mvgz",
    "outputId": "6fc80e51-9b0a-4ea3-c5ec-d90e5c08b690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3)\n",
      "(40000, 80)\n",
      "Train on 40000 samples, validate on 8000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 26s 661us/step - loss: 4.0309 - acc: 0.1215 - val_loss: 3.8603 - val_acc: 0.1863\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 25s 623us/step - loss: 3.1692 - acc: 0.2293 - val_loss: 3.0266 - val_acc: 0.2815\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 25s 622us/step - loss: 2.7898 - acc: 0.2998 - val_loss: 2.7705 - val_acc: 0.3245\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 25s 623us/step - loss: 2.5294 - acc: 0.3528 - val_loss: 2.2712 - val_acc: 0.4096\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 25s 623us/step - loss: 2.3256 - acc: 0.3948 - val_loss: 2.4311 - val_acc: 0.3920\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 25s 624us/step - loss: 2.1858 - acc: 0.4251 - val_loss: 2.1958 - val_acc: 0.4335\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 25s 623us/step - loss: 2.0821 - acc: 0.4448 - val_loss: 2.1525 - val_acc: 0.4386\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 25s 623us/step - loss: 1.9748 - acc: 0.4714 - val_loss: 2.1313 - val_acc: 0.4390\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 25s 622us/step - loss: 1.8923 - acc: 0.4943 - val_loss: 2.0119 - val_acc: 0.4842\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 25s 623us/step - loss: 1.8153 - acc: 0.5070 - val_loss: 1.9971 - val_acc: 0.4776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ef8892e80>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "print(x_training_CNN.shape)\n",
    "print(y_training_CNN.shape)\n",
    "model.fit(x_training_CNN, y_training_CNN,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data = (x_test_CNN, y_test_CNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "SRw5mU2MOplR",
    "outputId": "0dafcade-8e83-4627-ab89-3373045e63c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 245us/step\n",
      "Test loss: 1.9971144835948944\n",
      "Test accuracy: 0.477625\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test_CNN, y_test_CNN, verbose=1)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "r-trorOE3ArK",
    "outputId": "7bb43ab6-ce0d-4be6-8988-4206e0e1ec20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5_input (InputLayer)  (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 10, 10, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 5, 5, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 512)               1638912   \n",
      "=================================================================\n",
      "Total params: 1,899,840\n",
      "Trainable params: 1,899,456\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neural_codes_model = Model(inputs = model.input, outputs = model.get_layer(\"hidden_layer\").output)\n",
    "neural_codes_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ONjOoLqBE7s"
   },
   "outputs": [],
   "source": [
    "def make_oneshot_task2(N, X):\n",
    "    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    indices = np.random.randint(0, n_examples, size=(N,))\n",
    "    categories = np.random.choice(range(n_classes), size=(N,), replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
    "    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, d)\n",
    "    support_set = X[categories, indices, :, :]\n",
    "    support_set[0, :, :] = X[true_category, ex2]\n",
    "    support_set = support_set.reshape(N, w, h, d)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    #pairs = [test_image, support_set]\n",
    "    return test_image, support_set, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PgE2EVhABYop",
    "outputId": "d7249ddd-7d77-4ebd-b41a-0531e2a062d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got an average of 18.4% accuracy for 20-way one-shot learning\n"
     ]
    }
   ],
   "source": [
    "# One shot learning\n",
    "N=20\n",
    "n_correct = 0\n",
    "k=250\n",
    "\n",
    "for _ in range(0,250):\n",
    "    test_image, support_set, targets = make_oneshot_task2(N,x_train_rem_samples)\n",
    "    test_image_pred = neural_codes_model.predict(test_image)\n",
    "    support_set_pred = neural_codes_model.predict(support_set)\n",
    "    distances = []\n",
    "\n",
    "    for i in range(0,20):\n",
    "        dist = distance.euclidean(test_image_pred[i],support_set_pred[i])\n",
    "        distances.append(dist)\n",
    "    if np.argmin(distances) == np.argmax(targets):\n",
    "        n_correct += 1\n",
    "accuracy = (100.0*n_correct / k)\n",
    "print(\"Got an average of {}% accuracy for {}-way one-shot learning\".format(accuracy, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M1BDzdPAz26B"
   },
   "source": [
    "***\n",
    "\n",
    "**b)** Briefly motivate your CNN architecture, and discuss the difference in one-shot accuracy between the Siamese network approach and the CNN neural codes approach.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRpVm956FR8P"
   },
   "source": [
    "Initially we set the feature for convolution layers as 32, 64 and 128 and a Dropout of 0.2 to 0.5 was tried but could not achieve good accuracy. Then a conclusion was drawn that the features were not enough for it to train on. So started with 64 and 128 features in convolution layers so as to extract enough features. The maxpooling layer was added and dropouts were chosen to be 0.35 (except last layer) to avoid overfitting and also to reduce the density of the network. BatchNormalization was done in between these steps to normalize the output to increase the training speed. The hidden layer is fully connected dense layer with 512 neurons and last layer with 80 neurons for 80 output classes. The dropout is 0.5 so as to avoid overfitting and to remove the similar neurons. The activation is softmax as we need to perform classification. The optimizer adadelta continuously learns and is most suitable for classification tasks, it also works good for sparse networks. Trying with 10 epoch the model achieved an accuracy of 53% whereas with 30 epochs the model achieved an accuracy 56%. This result is due to extensive training of the model with more iterations.\n",
    "\n",
    "Siamese network achieved an accuracy of 16%. This network compares 2 images, their distances and concludes if they are from same class or not. Whereas the CNN uses L2 distance metric to measure the distance between embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-gkaM1tCThc"
   },
   "source": [
    "***\n",
    "## Question 2: Triplet networks & one-shot learning (10pt)\n",
    "\n",
    "### Task 2.1: Train a triplet network\n",
    "**a)**\n",
    "* Train a triplet network on the first 80 classes of (the training set of) Cifar-100.\n",
    " \n",
    "* Make sure the network achieves a smaller loss than the margin and the network does not collapse all representations to zero vectors. *HINT: If you experience problems to achieve this goal, it might be helpful to tinker the learning rate.*\n",
    "\n",
    "* You are provided with a working example of triplet loss implementation for Keras below. You may directly use it.\n",
    "\n",
    "You may ignore the test set of Cifar-100 for this question as well. It suffices to use only the training set and split this, using the first 80 classes for training and the remaining 20 classes for one-shot testing.\n",
    "\n",
    "```python\n",
    "# Notice that ground truth variable is not used for loss calculation. It is used as a function argument to by-pass some Keras functionality. This is because the network structure already implies the ground truth for the anchor image with the \"positive\" image.\n",
    "import tensorflow as tf\n",
    "def triplet_loss(ground_truth, network_output):\n",
    "\n",
    "    anchor, positive, negative = tf.split(network_output, num_or_size_splits=3, axis=1)        \n",
    "    \n",
    "    for embedding in [anchor, positive, negative]:\n",
    "        embedding = tf.math.l2_normalize(embedding)\n",
    "\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=1)\n",
    "    \n",
    "    margin = # define your margin\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), margin)\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), axis=0)\n",
    "\n",
    "    return loss\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "613lEuca7DKw"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def triplet_loss(ground_truth, network_output):\n",
    "\n",
    "    anchor, positive, negative = tf.split(network_output, num_or_size_splits=3, axis=1)        \n",
    "\n",
    "    for embedding in [anchor, positive, negative]:\n",
    "        embedding = tf.math.l2_normalize(embedding)\n",
    "\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=1)\n",
    "\n",
    "    margin = 0.15 # define your margin\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), margin)\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), axis=0)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1119
    },
    "colab_type": "code",
    "id": "GwrNzriE-vsZ",
    "outputId": "ba303731-0248-4f77-d2b1-7875c827237a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_89 (Conv2D)           (None, 30, 30, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_86 (Batc (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_90 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_87 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_91 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_92 (Conv2D)           (None, 2, 2, 256)         295168    \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer (Dense)         (None, 2048)              2099200   \n",
      "=================================================================\n",
      "Total params: 2,622,976\n",
      "Trainable params: 2,620,288\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_63 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_65 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_23 (Sequential)      (None, 2048)         2622976     input_63[0][0]                   \n",
      "                                                                 input_10[0][0]                   \n",
      "                                                                 input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 6144)         0           sequential_23[1][0]              \n",
      "                                                                 sequential_23[2][0]              \n",
      "                                                                 sequential_23[3][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,622,976\n",
      "Trainable params: 2,620,288\n",
      "Non-trainable params: 2,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "anchor_input = Input(input_shape)\n",
    "npositive_input = Input(input_shape)\n",
    "negative_input = Input(input_shape)\n",
    "\n",
    "# build convnet to use in each siamese 'leg'\n",
    "convnet_triplet = Sequential()\n",
    "\n",
    "convnet_triplet.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
    "convnet_triplet.add(MaxPooling2D())\n",
    "convnet_triplet.add(BatchNormalization())\n",
    "convnet_triplet.add(Dropout(0.25))\n",
    "\n",
    "convnet_triplet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet_triplet.add(MaxPooling2D())\n",
    "convnet_triplet.add(BatchNormalization())\n",
    "convnet_triplet.add(Dropout(0.25))\n",
    "\n",
    "convnet_triplet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet_triplet.add(BatchNormalization())\n",
    "convnet_triplet.add(Dropout(0.25))\n",
    "\n",
    "convnet_triplet.add(Conv2D(256, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
    "convnet_triplet.add(Flatten())\n",
    "convnet_triplet.add(BatchNormalization())\n",
    "convnet_triplet.add(Dropout(0.25))\n",
    "\n",
    "convnet_triplet.add(Dense(2048, activation=\"sigmoid\", name=\"hidden_layer\", kernel_regularizer=l2(1e-3)))\n",
    "convnet_triplet.summary()\n",
    "\n",
    "# encode each of the two inputs into a vector with the convnet\n",
    "encoded_anchor = convnet_triplet(anchor_input)\n",
    "encoded_positive = convnet_triplet(positive_input)\n",
    "encoded_negative = convnet_triplet(negative_input)\n",
    "\n",
    "# merge two encoded inputs with the L1 distance between them, and connect to prediction output layer\n",
    "# L2_distance = lambda x: K.abs(x[0]-x[1])\n",
    "\n",
    "# all_three = Lambda(L2_distance)([encoded_anchor, encoded_positive, encoded_negative])\n",
    "# prediction_t = Dense(1, activation='sigmoid')(all_three)\n",
    "\n",
    "merged_vector = Concatenate(axis=-1)([encoded_anchor, encoded_positive, encoded_negative])\n",
    "# prediction_t = Dense(6561, activation=\"sigmoid\", kernel_regularizer=l2(1e-3))(merged_vector)\n",
    "triplet_net = Model(inputs=[anchor_input, positive_input ,negative_input], outputs=merged_vector)\n",
    "# triplet_net = Model(inputs=[anchor_input, positive_input ,negative_input])\n",
    "# triplet_net.add(prediction_t)\n",
    "\n",
    "\n",
    "triplet_net.compile(loss=triplet_loss, optimizer=keras.optimizers.Adadelta(1.2))\n",
    "\n",
    "triplet_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fpWVI6fjSZbq"
   },
   "outputs": [],
   "source": [
    "def get_triplets(batch_size, X):\n",
    "    \"\"\"Create batch of n triplets, half same class, half different class\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "\n",
    "    categories = np.random.choice(n_classes, size=(batch_size,), replace=False)\n",
    "    triplets = [np.zeros((batch_size, h, w, d)) for i in range(3)]\n",
    "    targets = np.zeros((batch_size,))\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = np.random.randint(0, n_examples)\n",
    "        triplets[0][i, :, :, :] = X[category, idx_1].reshape(w, h, d)\n",
    "        idx_2 = np.random.randint(0, n_examples)\n",
    "        triplets[1][i, :, :, :] = X[category, idx_2].reshape(w, h, d)\n",
    "        idx_3 = np.random.randint(0, n_examples)\n",
    "        category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "        triplets[2][i, :, :, :] = X[category_2, idx_3].reshape(w, h, d)\n",
    "    return triplets, targets\n",
    "\n",
    "def triplet_generator(batch_size, X):\n",
    "    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "    while True:\n",
    "        triplets, targets = get_triplets(batch_size, X)\n",
    "        yield (triplets, targets)\n",
    "\n",
    "def train_triplet_network(model, X_train, batch_size=64, steps_per_epoch=100, epochs=40):\n",
    "    model.fit_generator(triplet_generator(batch_size, X_train), steps_per_epoch=steps_per_epoch, epochs=epochs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "0kkBc4ES1xeV",
    "outputId": "a395925b-85df-4ee6-beba-1758fd36fb06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 500, 32, 32, 3)\n",
      "(40000,)\n",
      "(20, 500, 32, 32, 3)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Data for triplet\n",
    "\n",
    "x_training_triplet = x_train_samples\n",
    "y_training_triplet = y_train_samples\n",
    "\n",
    "print(x_training_triplet.shape)\n",
    "print(y_training_triplet.shape)\n",
    "\n",
    "x_test_triplet = x_train_rem_samples\n",
    "y_test_triplet = y_train_rem_samples\n",
    "\n",
    "print(x_test_triplet.shape)\n",
    "print(y_test_triplet.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2221
    },
    "colab_type": "code",
    "id": "fRmiRbKplgzG",
    "outputId": "cf222896-bb98-4032-f20e-a5ef4681f961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 12.8759\n",
      "Epoch 2/60\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 12.4111\n",
      "Epoch 3/60\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 11.2317\n",
      "Epoch 4/60\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 10.8129\n",
      "Epoch 5/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 10.4428\n",
      "Epoch 6/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 10.1816\n",
      "Epoch 7/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 9.4674\n",
      "Epoch 8/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 9.0689\n",
      "Epoch 9/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 8.6481\n",
      "Epoch 10/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 7.7313\n",
      "Epoch 11/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 6.8906\n",
      "Epoch 12/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 5.3226\n",
      "Epoch 13/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 3.8487\n",
      "Epoch 14/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 2.6817\n",
      "Epoch 15/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 1.8796\n",
      "Epoch 16/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 1.3458\n",
      "Epoch 17/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.9996\n",
      "Epoch 18/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.7663\n",
      "Epoch 19/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.6127\n",
      "Epoch 20/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.5034\n",
      "Epoch 21/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.4298\n",
      "Epoch 22/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.3813\n",
      "Epoch 23/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.3481\n",
      "Epoch 24/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.3192\n",
      "Epoch 25/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.2957\n",
      "Epoch 26/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.2781\n",
      "Epoch 27/60\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.2663\n",
      "Epoch 28/60\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.2583\n",
      "Epoch 29/60\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.2446\n",
      "Epoch 30/60\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.2334\n",
      "Epoch 31/60\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.2274\n",
      "Epoch 32/60\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.2201\n",
      "Epoch 33/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.2136\n",
      "Epoch 34/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.2102\n",
      "Epoch 35/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.2047\n",
      "Epoch 36/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1978\n",
      "Epoch 37/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1948\n",
      "Epoch 38/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1892\n",
      "Epoch 39/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1895\n",
      "Epoch 40/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1854\n",
      "Epoch 41/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1851\n",
      "Epoch 42/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1783\n",
      "Epoch 43/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1781\n",
      "Epoch 44/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1728\n",
      "Epoch 45/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1741\n",
      "Epoch 46/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1686\n",
      "Epoch 47/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1668\n",
      "Epoch 48/60\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.1661\n",
      "Epoch 49/60\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.1623\n",
      "Epoch 50/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1657\n",
      "Epoch 51/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1622\n",
      "Epoch 52/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1599\n",
      "Epoch 53/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1565\n",
      "Epoch 54/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1607\n",
      "Epoch 55/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1560\n",
      "Epoch 56/60\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.1589\n",
      "Epoch 57/60\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.1535\n",
      "Epoch 58/60\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.1498\n",
      "Epoch 59/60\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.1547\n",
      "Epoch 60/60\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.1500\n"
     ]
    }
   ],
   "source": [
    "train_triplet_network(triplet_net, x_training_triplet, epochs=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHGJp45AR1qm"
   },
   "source": [
    "***\n",
    "\n",
    "### Task 2.2: One-shot learning with triplet neural codes\n",
    "**a)**\n",
    "* Use neural codes from the triplet network with L2-distance to evaluate one-shot learning accuracy for the remaining 20 classes of Cifar-100 with 250 random tasks. I.e. for a given one-shot task, obtain neural codes for the test image as well as the support set. Then pick the image from the support set that is closest (in L2-distance) to the test image as your one-shot prediction.\n",
    "* Explicitly state the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7w6o8xIXUADN"
   },
   "outputs": [],
   "source": [
    "def make_oneshot_task_triplet(N, X, c, language=None):\n",
    "    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n",
    "    n_classes, n_examples, w, h, d = X.shape\n",
    "    \n",
    "    indices = np.random.randint(0, n_examples, size=(N,))\n",
    "#     print\n",
    "    categories = np.random.choice(range(n_classes), size=(N,), replace=False)            \n",
    "    \n",
    "    true_category = categories[0]\n",
    "    \n",
    "    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
    "    \n",
    "    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, d)\n",
    "    \n",
    "    negative_set = X[categories, indices, :, :]\n",
    "#     support_set = X[categories, indices, :, :]\n",
    "#     positive_set = np.zeros((categories,indices,))\n",
    "    positive_set = np.asarray([X[true_category, ex2, :, :]]*N).reshape(N, w, h, d)\n",
    "    negative_set = negative_set.reshape(N, w, h, d)\n",
    "    targets = np.zeros((N,))\n",
    "    test_image, positive_set, negative_set = shuffle(test_image, positive_set, negative_set)\n",
    "    triplets = (test_image, positive_set, negative_set)\n",
    "    triplets = list(triplets)\n",
    "    return triplets\n",
    "\n",
    "def test_oneshot_triplet(model, X, c, N=20, k=250, language=None, verbose=True):\n",
    "    \"\"\"Test average N-way oneshot learning accuracy of a siamese neural net over k one-shot tasks.\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(k, N))\n",
    "    for i in range(k):\n",
    "        \n",
    "        triplets = make_oneshot_task_triplet(N, X, c, language=language)\n",
    "        probs = model.predict(triplets)\n",
    "        \n",
    "        loss = check_triple_net_loss(probs)\n",
    "        if loss > 0 :\n",
    "            n_correct +=1\n",
    "    percent_correct = (100.0*n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% accuracy for {}-way one-shot learning\".format(percent_correct, N))\n",
    "    return percent_correct\n",
    "\n",
    "\n",
    "def check_triple_net_loss(probs):\n",
    "    anchor, positive, negative = tf.split(probs, num_or_size_splits=3, axis=1)        \n",
    "\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=1)\n",
    "\n",
    "    actual_loss= tf.reduce_sum(tf.subtract(pos_dist,neg_dist)).eval(session = sess)\n",
    "    return actual_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "ZXIlFHsmb3eV",
    "outputId": "481d0337-68fe-4d15-8d40-5e79bb72f344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
      "Got an average of 16.8% accuracy for 20-way one-shot learning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.8"
      ]
     },
     "execution_count": 239,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.as_default()\n",
    "\n",
    "test_oneshot_triplet(triplet_net, x_test_triplet, [], N = 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCcmbz0UU7mR"
   },
   "source": [
    "***\n",
    "## Question 3: Performance comparison (3pt)\n",
    "\n",
    "\n",
    "**a)** What accuracy would random guessing achieve (on average) on this dataset? Motivate your answer briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BKGDydqsVVX1"
   },
   "source": [
    "On random guessing this data set would achieve (100/n_classes)% accuracy. Here we do it for 20 classes so 5% is the accuracy that will be achieved for random guessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KLXRv-eV04Q"
   },
   "source": [
    "**b)** Discuss and compare the performances of networks in tasks 1.1, 1.2 and 2.2. Briefly motivate and explain which task would be expected the highest accuracy. Explain the reasons of the accuracy difference if there are any. If there is almost no difference accuracy, explain the reason for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "71kTHFBkcjp8"
   },
   "source": [
    "The performance of the tasks vary for all the three questions. Q1.1 gives an accuracy of around 15%, Q1.2 gives an accuracy of around 48% and Q2.2 gives an accuracy of  16.8%. In 1.1 and 2.2 we compare the similarity between images with respect to their distances and say if they are from same class or not. Since it is colour image with varying background and has some noise in it, similarity calculation of final image performs poorly but 1.2 calculates the distance between the two images with help of the embedding matrix and says if it is from similar class or not. This variation of metric costs big and thus affect the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pn1Q0zG1Mvg_"
   },
   "source": [
    "***\n",
    "## Question 4: Peer review (0pt)\n",
    "\n",
    "Finally, each group member must write a single paragraph outlining their opinion on the work distribution within the group. Did every group member contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vL2NQVFxMvg_"
   },
   "source": [
    "*=== write your answer here ===*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_2_FinalFinal.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
