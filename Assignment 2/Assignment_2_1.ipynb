{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uj4T8PEHGbMF"
      },
      "source": [
        "# Assignment 2\n",
        "## Question 1: Siamese networks & one-shot learning (7pt)\n",
        "The Cifar-100 dataset is similar to the Cifar-10 dataset. It also consists of 60,000 32x32 RGB images, but they are distributed over 100 classes instead of 10. Thus, each class has much fewer examples, only 500 training images and 100 testing images per class. For more info about the dataset, see https://www.cs.toronto.edu/~kriz/cifar.html.\n",
        "\n",
        "*HINT: Import the Cifar-100 dataset directly from Keras, no need to download it from the website. Use* `label_mode=\"fine\"`\n",
        "\n",
        "### Task 1.1: Siamese network\n",
        "**a)**\n",
        "* Train a Siamese Network on the first 80 classes of (the training set of) Cifar-100, i.e. let the network predict the probability that two input images are from the same class. Use 1 as a target for pairs of images from the same class (positive pairs), and 0 for pairs of images from different classes (negative pairs). Randomly select image pairs from Cifar-100, but make sure you train on as many positive pairs as negative pairs.\n",
        "\n",
        "* Evaluate the performance of the network on 20-way one-shot learning tasks. Do this by generating 250 random tasks and obtain the average accuracy for each evaluation round. Use the remaining 20 classes that were not used for training. The model should perform better than random guessing.\n",
        "\n",
        "For this question you may ignore the test set of Cifar-100; it suffices to use only the training set and split this, using the first 80 classes for training and the remaining 20 classes for one-shot testing.\n",
        "\n",
        "*HINT: First sort the data by their labels (see e.g.* `numpy.argsort()`*), then reshape the data to a shape of* `(n_classes, n_examples, width, height, depth)`*, similar to the Omniglot data in Practical 4. It is then easier to split the data by class, and to sample positive and negative images pairs for training the Siamese network.*\n",
        "\n",
        "*NOTE: do not expect the one-shot accuracy for Cifar-100 to be similar to that accuracy for Omniglot; a lower accuracy can be expected. However, accuracy higher than random guess is certainly achievable.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5XlMo9yMvf5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1998bba8-a440-4080-f817-404120bd8d53"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import cifar100\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from keras import backend as K\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MaoAaEv17v6k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "df2c357e-ac1e-4ca8-efae-9341bafd7ec3"
      },
      "source": [
        "# === add code here ===\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode=\"fine\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 20s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiaEnHNMMvgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "534c7578-9f27-4255-d054-f403d113de67"
      },
      "source": [
        "print(x_train.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JdsjUSzMvgD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "75f7b282-aef6-4903-f3df-cb7290d2d9c4"
      },
      "source": [
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgoiy9tqMvgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_labels = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
        "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
        "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
        "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
        "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
        "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
        "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
        "    'worm']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aVrASEXMvgH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "66582a6f-6bc0-4235-8601-01c227d6648a"
      },
      "source": [
        "example_id = 39999  # pick any integer from 0 to 49999 to visualize a training example\n",
        "example = x_train[example_id]\n",
        "label = y_train[example_id]\n",
        "print(\"Class label:\", class_labels[label])\n",
        "plt.imshow(example)\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class label: skunk\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coyB9jvFMvgJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "da5c706e-8551-4eaa-8bea-a187f173377d"
      },
      "source": [
        "img_rows,img_cols,chns = 32, 32, 3\n",
        "n_classes = 100\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, chns)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, chns)\n",
        "input_shape = (img_rows, img_cols, chns)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000,)\n",
            "(10000, 32, 32, 3)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNBIWXg9MvgM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f660759-22a5-43bc-809c-4fea6885ce9a"
      },
      "source": [
        "indexes = y_train.argsort()\n",
        "print(indexes)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[42957  2126 43210 ... 42095 26322 21122]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y6mh6ZYMvgP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "67972d46-2b57-42bc-f5fd-ac53c3f80474"
      },
      "source": [
        "x_train_sort = x_train[indexes]\n",
        "y_train_sort = y_train[indexes]\n",
        "\n",
        "print(x_train_sort.shape)\n",
        "print(y_train_sort.shape)\n",
        "\n",
        "example_id = 39999  # pick any integer from 0 to 49999 to visualize a training example\n",
        "example = x_train_sort[example_id]\n",
        "label = y_train_sort[example_id]\n",
        "print(\"Class label:\", class_labels[label])\n",
        "plt.imshow(example)\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000,)\n",
            "Class label: spider\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCsrdPYcMvgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def group(a, b):\n",
        "    # Get argsort indices, to be used to sort a and b in the next steps\n",
        "    sidx = b.argsort(kind='mergesort')\n",
        "    a_sorted = a[sidx]\n",
        "    b_sorted = b[sidx]\n",
        "\n",
        "    # Get the group limit indices (start, stop of groups)\n",
        "    cut_idx = np.flatnonzero(np.r_[True,b_sorted[1:] != b_sorted[:-1],True])\n",
        "\n",
        "    # Create cut indices for all unique IDs in b\n",
        "    n = b_sorted[-1]+2\n",
        "    cut_idxe = np.full(n, cut_idx[-1], dtype=int)\n",
        "\n",
        "    insert_idx = b_sorted[cut_idx[:-1]]\n",
        "    cut_idxe[insert_idx] = cut_idx[:-1]\n",
        "    cut_idxe = np.minimum.accumulate(cut_idxe[::-1])[::-1]\n",
        "\n",
        "    # Split input array with those start, stop ones\n",
        "    out = [a_sorted[i:j] for i,j in zip(cut_idxe[:-1],cut_idxe[1:])]\n",
        "    return np.array(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf0wCf1nMvgW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2084ddad-7910-453c-b586-1771462c0ffd"
      },
      "source": [
        "x_train_shape = group(x_train_sort, y_train_sort)\n",
        "x_test_shape = group(x_test, y_test)\n",
        "x_test.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8u0pBHZMvgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "cbdd0d32-18a8-4cbe-a53d-40eccb582288"
      },
      "source": [
        "print(len(x_train_shape))\n",
        "x_train_samples = x_train_shape[:80]\n",
        "y_train_samples = y_train_sort[:80]\n",
        "print(len(x_train_samples))\n",
        "print(x_train_samples.shape)\n",
        "print(y_train_samples.shape)\n",
        "print(x_test_shape.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "80\n",
            "(80, 500, 32, 32, 3)\n",
            "(80,)\n",
            "(100, 100, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE5d-Oq7Mvgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1112
        },
        "outputId": "947e0582-f43b-450c-e480-1841fca9353f"
      },
      "source": [
        "input_shape = (32, 32, 3)\n",
        "left_input = Input(input_shape)\n",
        "right_input = Input(input_shape)\n",
        "\n",
        "# build convnet to use in each siamese 'leg'\n",
        "convnet = Sequential()\n",
        "convnet.add(Conv2D(64, (3,3), activation='relu', input_shape=input_shape, kernel_regularizer=l2(2e-4)))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(Dropout(0.25))\n",
        "convnet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(Dropout(0.25))\n",
        "convnet.add(Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "#convnet.add(MaxPooling2D())\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(Dropout(0.25))\n",
        "convnet.add(Conv2D(256, (3,3), activation='relu', kernel_regularizer=l2(2e-4)))\n",
        "convnet.add(Flatten())\n",
        "convnet.add(BatchNormalization())\n",
        "convnet.add(Dropout(0.25))\n",
        "convnet.add(Dense(4096, activation=\"sigmoid\", kernel_regularizer=l2(1e-3)))\n",
        "convnet.summary()\n",
        "\n",
        "# encode each of the two inputs into a vector with the convnet\n",
        "encoded_l = convnet(left_input)\n",
        "encoded_r = convnet(right_input)\n",
        "\n",
        "# merge two encoded inputs with the L1 distance between them, and connect to prediction output layer\n",
        "L1_distance = lambda x: K.abs(x[0]-x[1])\n",
        "both = Lambda(L1_distance)([encoded_l, encoded_r])\n",
        "prediction = Dense(1, activation='sigmoid')(both)\n",
        "siamese_net = Model(inputs=[left_input,right_input], outputs=prediction)\n",
        "\n",
        "\n",
        "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "\n",
        "siamese_net.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 30, 30, 64)        1792      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 15, 15, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 15, 15, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 13, 13, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 2, 2, 256)         295168    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              4198400   \n",
            "=================================================================\n",
            "Total params: 4,722,176\n",
            "Trainable params: 4,719,488\n",
            "Non-trainable params: 2,688\n",
            "_________________________________________________________________\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 4096)         4722176     input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 4096)         0           sequential_1[1][0]               \n",
            "                                                                 sequential_1[2][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 4,726,273\n",
            "Trainable params: 4,723,585\n",
            "Non-trainable params: 2,688\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRpbGhM4Mvgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(batch_size, X):\n",
        "    \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
        "    n_classes, n_examples, w, h, d = X.shape\n",
        "    # randomly sample several classes to use in the batch\n",
        "    categories = np.random.choice(n_classes, size=(batch_size,), replace=False)\n",
        "    # initialize 2 empty arrays for the input image batch\n",
        "    pairs = [np.zeros((batch_size, h, w, d)) for i in range(2)]\n",
        "    # initialize vector for the targets, and make one half of it '1's, so 2nd half of batch has same class\n",
        "    targets = np.zeros((batch_size,))\n",
        "    targets[batch_size//2:] = 1\n",
        "    for i in range(batch_size):\n",
        "        category = categories[i]\n",
        "        idx_1 = np.random.randint(0, n_examples)\n",
        "        pairs[0][i, :, :, :] = X[category, idx_1].reshape(w, h, d)\n",
        "        idx_2 = np.random.randint(0, n_examples)\n",
        "        # pick images of same class for 1st half, different for 2nd\n",
        "        if i >= batch_size // 2:\n",
        "            category_2 = category\n",
        "        else:\n",
        "            #add a random number to the category modulo n_classes to ensure 2nd image has different category\n",
        "            category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
        "        pairs[1][i, :, :, :] = X[category_2,idx_2].reshape(w, h, d)\n",
        "    return pairs, targets\n",
        "\n",
        "def batch_generator(batch_size, X):\n",
        "    \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
        "    while True:\n",
        "        pairs, targets = get_batch(batch_size, X)\n",
        "        yield (pairs, targets)\n",
        "\n",
        "def train(model, X_train, batch_size=64, steps_per_epoch=100, epochs=10):\n",
        "    model.fit_generator(batch_generator(batch_size, X_train), steps_per_epoch=steps_per_epoch, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyhiiquKMvgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "3173f244-5cd1-4f2a-9415-0cfffe80644d"
      },
      "source": [
        "train(siamese_net, x_train_samples)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 11s 113ms/step - loss: 1.8693\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 1.2813\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.9959\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.8697\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.8064\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.7759\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.7641\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.7550\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.7420\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.7283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QBNX9nXMvgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_oneshot_task(N, X, c, language=None):\n",
        "    \"\"\"Create pairs of (test image, support set image) with ground truth, for testing N-way one-shot learning.\"\"\"\n",
        "    n_classes, n_examples, w, h, d = X.shape\n",
        "    indices = np.random.randint(0, n_examples, size=(N,))\n",
        "    if language is not None:\n",
        "        low, high = c[language]\n",
        "        if N > high - low:\n",
        "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
        "        categories = np.random.choice(range(low,high), size=(N,), replace=False)\n",
        "    else:  # if no language specified just pick a bunch of random letters\n",
        "        categories = np.random.choice(range(n_classes), size=(N,), replace=False)            \n",
        "    true_category = categories[0]\n",
        "    ex1, ex2 = np.random.choice(n_examples, replace=False, size=(2,))\n",
        "    test_image = np.asarray([X[true_category, ex1, :, :]]*N).reshape(N, w, h, d)\n",
        "    support_set = X[categories, indices, :, :]\n",
        "    support_set[0, :, :] = X[true_category, ex2]\n",
        "    support_set = support_set.reshape(N, w, h, d)\n",
        "    targets = np.zeros((N,))\n",
        "    targets[0] = 1\n",
        "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
        "    pairs = [test_image, support_set]\n",
        "    return pairs, targets\n",
        "\n",
        "def test_oneshot(model, X, c, N=20, k=250, language=None, verbose=True):\n",
        "    \"\"\"Test average N-way oneshot learning accuracy of a siamese neural net over k one-shot tasks.\"\"\"\n",
        "    n_correct = 0\n",
        "    if verbose:\n",
        "        print(\"Evaluating model on {} random {}-way one-shot learning tasks ...\".format(k, N))\n",
        "    for i in range(k):\n",
        "        inputs, targets = make_oneshot_task(N, X, c, language=language)\n",
        "        probs = model.predict(inputs)\n",
        "        if np.argmax(probs) == np.argmax(targets):\n",
        "            n_correct += 1\n",
        "    percent_correct = (100.0*n_correct / k)\n",
        "    if verbose:\n",
        "        print(\"Got an average of {}% accuracy for {}-way one-shot learning\".format(percent_correct, N))\n",
        "    return percent_correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOGl4uGVMvgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs, targets = make_oneshot_task(20, x_train_samples, y_train_samples)\n",
        "#plot_oneshot_task(pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwQPPcsHMvgn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "outputId": "f68699ad-fe93-4474-e528-05b9931832d2"
      },
      "source": [
        "best_acc = 0\n",
        "train(siamese_net, x_train_samples)\n",
        "test_acc = test_oneshot(siamese_net, x_test_shape, y_test)\n",
        "if test_acc >= best_acc:\n",
        "    print(\"New best one-shot accuracy, saving model ...\")\n",
        "    siamese_net.save(os.path.join(\"models\", \"siamese_omniglot.h5\"))\n",
        "    best_acc = test_acc"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "100/100 [==============================] - 6s 60ms/step - loss: 0.7269\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.7223\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.7163\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 5s 55ms/step - loss: 0.7130\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.7091\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.7114\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.7039\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.7048\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.7043\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.7064\n",
            "Evaluating model on 250 random 20-way one-shot learning tasks ...\n",
            "Got an average of 13.6% accuracy for 20-way one-shot learning\n",
            "New best one-shot accuracy, saving model ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-368a4329eb72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"New best one-shot accuracy, saving model ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msiamese_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"siamese_omniglot.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'models/siamese_omniglot.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "COiAqXWDAgCe"
      },
      "source": [
        "***\n",
        "\n",
        "**b)** Compare the performance of your Siamese network for Cifar-100 to the Siamese network from Practical 4 for Omniglot. Name three fundamental differences between the Cifar-100 and Omniglot datasets. How do these differences influence the difference in one-shot accuracy?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvpbMD4ZMvgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IIHkoQ0PBWuB"
      },
      "source": [
        "*=== write your answer here ===*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VWpFF_5-Bf4B"
      },
      "source": [
        "***\n",
        "\n",
        "### Task 1.2: One-shot learning with neural codes\n",
        "**a)**\n",
        "* Train a CNN classifier on the first 80 classes of Cifar-100. Make sure it achieves at least 40% classification accuracy on those 80 classes (use the test set to validate this accuracy).\n",
        "* Then use neural codes from one of the later hidden layers of the CNN with L2-distance to evaluate one-shot learning accuracy for the remaining 20 classes of Cifar-100 with 250 random tasks. I.e. for a given one-shot task, obtain neural codes for the test image as well as the support set. Then pick the image from the support set that is closest (in L2-distance) to the test image as your one-shot prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n9sICFTMvgu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "853bbdb4-d5d5-4ef1-fbe9-0326188bc45f"
      },
      "source": [
        "x_training_CNN = x_train_sort[:40000]\n",
        "y_training = y_train_sort[:40000]\n",
        "\n",
        "y_training_CNN = to_categorical(y_training, 100)\n",
        "y_test_CNN = to_categorical(y_test, 100)\n",
        "\n",
        "print(x_training_CNN.shape)\n",
        "print(y_training_CNN.shape)\n",
        "print(y_test_CNN.shape)\n",
        "\n",
        "example_id = 39999  # pick any integer from 0 to 49999 to visualize a training example\n",
        "example = x_training_CNN[example_id]\n",
        "label = y_train_sort[example_id]\n",
        "print(\"Class label:\", class_labels[label])\n",
        "plt.imshow(example)\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 32, 32, 3)\n",
            "(40000, 100)\n",
            "(10000, 100)\n",
            "Class label: spider\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qU7rNa157v6r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "b1bf0ff9-eaa5-4d01-f795-6924a79cb900"
      },
      "source": [
        "# === add code here ===\n",
        "input_shape = (32, 32, 3)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32,(3,3),activation = 'relu', input_shape = input_shape))\n",
        "#model.add(Conv2D(32,(3,3),activation = 'relu'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
        "#model.add(Conv2D(64,(3,3),activation = 'relu'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Conv2D(256,(3,3),activation = 'relu'))\n",
        "#model.add(Conv2D(128,(3,3),activation = 'relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.4))\n",
        "model.add(Flatten())\n",
        "#model.add(Conv2D(256,(3,3),activation = 'relu'))\n",
        "#model.add(Conv2D(256,(3,3),activation = 'relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.25))\n",
        "model.add(Dense(100,activation = 'softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=\"adadelta\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_38 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 15, 15, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_37 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 6, 6, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 100)               230500    \n",
            "=================================================================\n",
            "Total params: 250,276\n",
            "Trainable params: 250,084\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNh4qrV1Mvgz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "3eb8b5db-c53b-4c02-c3c1-0d32769260d9"
      },
      "source": [
        "batch_size = 100\n",
        "epochs = 10\n",
        "print(x_training_CNN.shape)\n",
        "print(y_training_CNN.shape)\n",
        "model.fit(x_training_CNN, y_training_CNN,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data = (x_test, y_test_CNN))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 32, 32, 3)\n",
            "(40000, 100)\n",
            "Train on 36000 samples, validate on 4000 samples\n",
            "Epoch 1/10\n",
            "36000/36000 [==============================] - 6s 161us/step - loss: 2.0823 - acc: 0.4521 - val_loss: 7.2763 - val_acc: 0.0015\n",
            "Epoch 2/10\n",
            "36000/36000 [==============================] - 6s 159us/step - loss: 2.0387 - acc: 0.4620 - val_loss: 8.7795 - val_acc: 2.5000e-04\n",
            "Epoch 3/10\n",
            "36000/36000 [==============================] - 6s 159us/step - loss: 2.0179 - acc: 0.4666 - val_loss: 9.9769 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "36000/36000 [==============================] - 6s 159us/step - loss: 1.9966 - acc: 0.4685 - val_loss: 11.0055 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "36000/36000 [==============================] - 6s 158us/step - loss: 1.9804 - acc: 0.4760 - val_loss: 12.2723 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "36000/36000 [==============================] - 6s 157us/step - loss: 1.9539 - acc: 0.4767 - val_loss: 9.8306 - val_acc: 5.0000e-04\n",
            "Epoch 7/10\n",
            "36000/36000 [==============================] - 6s 156us/step - loss: 1.9391 - acc: 0.4863 - val_loss: 12.1956 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "36000/36000 [==============================] - 6s 156us/step - loss: 1.9251 - acc: 0.4872 - val_loss: 13.5603 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "36000/36000 [==============================] - 6s 156us/step - loss: 1.9038 - acc: 0.4895 - val_loss: 14.8137 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "36000/36000 [==============================] - 6s 156us/step - loss: 1.8870 - acc: 0.4941 - val_loss: 14.2097 - val_acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f02dbaed550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRw5mU2MOplR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "5fce07b5-180d-4295-e0a0-afde4c534904"
      },
      "source": [
        "loss, accuracy = model.evaluate(x_test, y_test_CNN, verbose=1)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 123us/step\n",
            "Test loss: 6.1813901512145994\n",
            "Test accuracy: 0.2505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M1BDzdPAz26B"
      },
      "source": [
        "***\n",
        "\n",
        "**b)** Briefly motivate your CNN architecture, and discuss the difference in one-shot accuracy between the Siamese network approach and the CNN neural codes approach.\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oRpVm956FR8P"
      },
      "source": [
        "*=== write your answer here ===*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p-gkaM1tCThc"
      },
      "source": [
        "***\n",
        "## Question 2: Triplet networks & one-shot learning (10pt)\n",
        "\n",
        "### Task 2.1: Train a triplet network\n",
        "**a)**\n",
        "* Train a triplet network on the first 80 classes of (the training set of) Cifar-100.\n",
        " \n",
        "* Make sure the network achieves a smaller loss than the margin and the network does not collapse all representations to zero vectors. *HINT: If you experience problems to achieve this goal, it might be helpful to tinker the learning rate.*\n",
        "\n",
        "* You are provided with a working example of triplet loss implementation for Keras below. You may directly use it.\n",
        "\n",
        "You may ignore the test set of Cifar-100 for this question as well. It suffices to use only the training set and split this, using the first 80 classes for training and the remaining 20 classes for one-shot testing.\n",
        "\n",
        "```python\n",
        "# Notice that ground truth variable is not used for loss calculation. It is used as a function argument to by-pass some Keras functionality. This is because the network structure already implies the ground truth for the anchor image with the \"positive\" image.\n",
        "import tensorflow as tf\n",
        "def triplet_loss(ground_truth, network_output):\n",
        "\n",
        "    anchor, positive, negative = tf.split(network_output, num_or_size_splits=3, axis=1)        \n",
        "    \n",
        "    for embedding in [anchor, positive, negative]:\n",
        "        embedding = tf.math.l2_normalize(embedding)\n",
        "\n",
        "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=1)\n",
        "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=1)\n",
        "    \n",
        "    margin = # define your margin\n",
        "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), margin)\n",
        "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), axis=0)\n",
        "\n",
        "    return loss\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FIHD8l_rRLy5",
        "colab": {}
      },
      "source": [
        "# === add code here ==="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XHGJp45AR1qm"
      },
      "source": [
        "***\n",
        "\n",
        "### Task 2.2: One-shot learning with triplet neural codes\n",
        "**a)**\n",
        "* Use neural codes from the triplet network with L2-distance to evaluate one-shot learning accuracy for the remaining 20 classes of Cifar-100 with 250 random tasks. I.e. for a given one-shot task, obtain neural codes for the test image as well as the support set. Then pick the image from the support set that is closest (in L2-distance) to the test image as your one-shot prediction.\n",
        "* Explicitly state the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7w6o8xIXUADN",
        "colab": {}
      },
      "source": [
        "# === add code here ==="
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CCcmbz0UU7mR"
      },
      "source": [
        "***\n",
        "## Question 3: Performance comparison (3pt)\n",
        "\n",
        "\n",
        "**a)** What accuracy would random guessing achieve (on average) on this dataset? Motivate your answer briefly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BKGDydqsVVX1"
      },
      "source": [
        "*=== write your answer here ===*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5KLXRv-eV04Q"
      },
      "source": [
        "**b)** Discuss and compare the performances of networks in tasks 1.1, 1.2 and 2.2. Briefly motivate and explain which task would be expected the highest accuracy. Explain the reasons of the accuracy difference if there are any. If there is almost no difference accuracy, explain the reason for that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "71kTHFBkcjp8"
      },
      "source": [
        "*=== write your answer here ===*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn1Q0zG1Mvg_",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "## Question 4: Peer review (0pt)\n",
        "\n",
        "Finally, each group member must write a single paragraph outlining their opinion on the work distribution within the group. Did every group member contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL2NQVFxMvg_",
        "colab_type": "text"
      },
      "source": [
        "*=== write your answer here ===*"
      ]
    }
  ]
}