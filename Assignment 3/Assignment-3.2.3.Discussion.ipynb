{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3.2. Image Caption Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 09\n",
    "Abhishek Mahadevan Raju (1306162), Priya Sivasubramanian (1378635), Natarajan Chidambaram (1358111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Briefly describe one of the preceding works on modeling Image-Caption according to the paper and its limitation.  Name the advantage(s) of the current Image-Caption generator as compared to the previous work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model use dependency tree RNN (DT-RNN) to get the meaningful vector representations for sentences. The learning is done by making the resulting vectors as multimodal features and in the same space both the image and sentence mapping are jointly done. The evaluation is done by comparing the ability of DT-RNN to find the relevant image for the given query sentence and also to give sentence representation for the query image given to it.\n",
    "\n",
    "The longer phrase representation for single word vector are done by averaging the word vectors. The active passive formalism of sentences are done to drop/include some words. The neural network constructed will output a high weight for documents and windows that occur in unlabeled corpus while it outputs a lower weight for windows-document pair where one word is replaced by a random word. To predict the correct weight, the vectors in the matrix capture co-occurance statistics. So the sentence is now a sequence with ordered list of word-vector pairs. Through forward propagation, the DT-RNN computes the parent vectors.\n",
    "\n",
    "This has the advantage of finding the single words that are important for the tasks like sentiment analysis, keep them as hidden layer and contruct the exact parent vector from these hidden vectors (words) rather than going the other way. This allows n-ary nodes in the tree. The final representation of the image in sentence form is more robust to less important adjectival modifiers and word order changes.\n",
    "\n",
    "Limitations is, when one image is represented using sentences, its constructed in a different form. One of them use verbs, other does not, one is more descriptive, the other is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. How does the model extract image features from raw images? (inspect the provided code preprocessing1.0.py, preprocessing2.0.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses the image encoding obtained by preprocessing the raw images from flickr dataset using a pretrained model namely \"inceptionv3\". Inception v3 is a widely-used image recognition model that has been shown to attain greater than 78.1% accuracy on the ImageNet dataset. \n",
    "During preprocessing, a new model is built by removing one of the last hidden layers from the inceptionv3. Image encoding for each image in our flickr dataset is obtained by using the new model and the raw images of the flickr dataset as input, and stored in a pickle file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. How are the train descriptions represented into the model? Why do we need to add \"\\starting\" and \"\\ending\" token of every caption in a preprocessing stage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train descriptions have the information about the subjects and actions in the images and there are 5 captions for each image. This is preprocessed inorder to insert a startseq and endseq to each caption. Later a vocabulary is created from all the words which occur at least 10 times in the whole corpus. Embedding matrix for each word in the vocabulary is obtained based on the corresponding word vectors from the GloVe vectors (Global vectors for word representation).Here we use startseq and endseq in each caption to uniquely identify the start of each caption as well as the end of each caption which is later used in the greedy and beam search. This is done since the exact input to the model is an image and a sequence, and the model predicts the probabilities of the next words. Having a starting token allows us to enable to starting point for recursively calling the model to predict the next word. Having an ending token allows us to stop the prediction of words, by denoting an ending feature to all captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the motivation of incorporating Beam Search in Sequence-to-Sequence learning? Briefly explain how the method works in an inference stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beam-search allows the caption generation to consider a larger scope of possibilities, compared to the greedy approach. The greedy approach performs the following operation - at each step, it takes the word that the model states to have the highest probability of being next, given the sequence generated so far. The greedy approach is thus narrow - it has to make an arbitrary choice if two words are predicted with equally high probability, and certain choices might lead to a sub-optimal output. The beam-search method allows us to circumvent this problem, by considering a certain number (called beam size) of the top choices at every step-layer. A larger beam size allows us to consider more paths that might lead to more optimal solutions, by ending up with more probable solutions. This is demonstrated in our IPython notebook, for the example test picture - The greedy approach says that 'A dog is jumping over a fence', while the Beam Search method describes more precisely that 'The german shepherd dog is running through the grass'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Doc_level_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
