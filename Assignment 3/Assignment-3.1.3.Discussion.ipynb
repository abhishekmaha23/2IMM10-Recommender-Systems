{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3.1. Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group 09\n",
    "\n",
    "Abhishek Mahadevan Raju (1306162), Priya Sivasubramanian (1378635), Natarajan Chidambaram (1358111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the motivation of incorporating an \"attention mechanism\" in a Machine Translation task? What is the main issue that this attention trying to solve? Mention the advantage(s) as compared to the model without attention.\n",
    "\n",
    "\n",
    "2. Likewise, what is the motivation of adding an \"attention\" network in aspect-level sentiment classification? What is the main issue that this attention trying to solve? Mention the advantage(s) as compared to the model without attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "In the machine translation task, we need to encode the input sequence to a fixed length vector (context vector) from which we need to decode each output time step. This is complex and time consuming when it comes to long sequences or sequences with many important information in it. Thus attention mechanim is used to overcome this. In machine translation we need to know which part of the input sequence corresponds to the output word and which words will play a role in selecting the appropriate output. This is called alignment and translation and this can be done with attention mechanism. Thus using attention mechanism we can get the context vector that corresponds to each output time step. Attention mechanism also makes use of all the intermediate encoder states in order to make the decoder predict the output optimally. With this we can translate longer sequences with lesser training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "For sentiment classification we need to concentrate not only on content but also on context and aspect. Attention mechanism can distinguish between the different aspects when a long a sequence is given as input. Instead of considering a bag of words and giving sentiment to them individually irrespective of the context, this helps in idenfying the aspect and give the polarity as well as the ranking based on the context of the test. This also helps in achieving better accuracy than the encoder-decoder LSTM model and remebers more from history. Attention mechanism helps in identifying the categories of the aspect present in the sequence and predict the sentiment polarity more apropriately than the model without attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Doc_level_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
